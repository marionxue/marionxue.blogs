{"categories":[],"posts":[{"content":"导读 本文介绍了一些可以用来监控网络使用情况的Linux命令行工具。这些工具可以监控通过网络接口传输的数据，并测量目前哪些数据所传输的速度。入站流量和出站流量分开来显示。\n一些命令可以显示单个进程所使用的带宽。这样一来，用户很容易发现过度使用网络带宽的某个进程。\n这些工具使用不同的机制来制作流量报告。nload等一些工具可以读取\u0026quot;proc/net/dev\u0026quot;文件，以获得流量统计信息；而一些工具使用pcap库来捕获所有数据包，然后计算总数据量，从而估计流量负载。\n下面是按功能划分的命令名称。\n 监控总体带宽使用――nload、bmon、slurm、bwm-ng、cbm、speedometer和netload 监控总体带宽使用（批量式输出）――vnstat、ifstat、dstat和collectl 每个套接字连接的带宽使用――iftop、iptraf、tcptrack、pktstat、netwatch和trafshow 每个进程的带宽使用――nethogs  1、nload nload是一个命令行工具，让用户可以分开来监控入站流量和出站流量。它还可以绘制图表以显示入站流量和出站流量，视图比例可以调整。用起来很简单，不支持许多选项。另外关注公众号码猿技术专栏，回复关键词“面试宝典”，送你一份阿里内部面试资料！\n所以，如果你只需要快速查看总带宽使用情况，无需每个进程的详细情况，那么nload用起来很方便。\n安装nload：Fedora和Ubuntu在默认软件库里面就有nload。CentOS用户则需要从Epel软件库获得nload。\n2、iftop iftop可测量通过每一个套接字连接传输的数据；它采用的工作方式有别于nload。iftop使用pcap库来捕获进出网络适配器的数据包，然后汇总数据包大小和数量，搞清楚总的带宽使用情况。\n虽然iftop报告每个连接所使用的带宽，但它无法报告参与某个套按字连接的进程名称/编号（ID）。不过由于基于pcap库，iftop能够过滤流量，并报告由过滤器指定的所选定主机连接的带宽使用情况。\nn选项可以防止iftop将IP地址解析成主机名，解析本身就会带来额外的网络流量。\n安装iftop：Ubuntu/Debian/Fedora用户可以从默认软件库获得它。CentOS用户可以从Epel获得它。\n3、iptraf iptraf是一款交互式、色彩鲜艳的IP局域网监控工具。它可以显示每个连接以及主机之间传输的数据量。下面是屏幕截图。\n安装iptraf：\n4、nethogs nethogs是一款小巧的\u0026quot;net top\u0026quot;工具，可以显示每个进程所使用的带宽，并对列表排序，将耗用带宽最多的进程排在最上面。万一出现带宽使用突然激增的情况，用户迅速打开nethogs，就可以找到导致带宽使用激增的进程。nethogs可以报告程序的进程编号（PID）、用户和路径。另外关注公众号码猿技术专栏，回复关键词“面试宝典”，送你一份阿里内部面试资料！\n安装nethogs：Ubuntu、Debian和Fedora用户可以从默认软件库获得。CentOS用户则需要Epel。\n5. bmon bmon（带宽监控器）是一款类似nload的工具，它可以显示系统上所有网络接口的流量负载。输出结果还含有图表和剖面，附有数据包层面的详细信息。\n安装bmon：Ubuntu、Debian和Fedora用户可以从默认软件库来安装。CentOS用户则需要安装repoforge，因为Epel里面没有bmon。\n6. slurm slurm是另一款网络负载监控器，可以显示设备的统计信息，还能显示ASCII图形。它支持三种不同类型的图形，使用c键、s键和l键即可激活每种图形。slurm功能简单，无法显示关于网络负载的任何更进一步的详细信息。\n安装slurm\n7. tcptrack tcptrack类似iftop，使用pcap库来捕获数据包，并计算各种统计信息，比如每个连接所使用的带宽。它还支持标准的pcap过滤器，这些过滤器可用来监控特定的连接。\n安装tcptrack：Ubuntu、Debian和Fedora在默认软件库里面就有它。CentOS用户则需要从RepoForge获得它，因为Epel里面没有它。\n8. vnstat vnstat与另外大多数工具有点不一样。它实际上运行后台服务/守护进程，始终不停地记录所传输数据的大小。之外，它可以用来制作显示网络使用历史情况的报告\n运行没有任何选项的vnstat，只会显示自守护进程运行以来所传输的数据总量。\n想实时监控带宽使用情况，请使用\u0026rdquo;-l\u0026quot;选项（实时模式）。然后，它会显示入站数据和出站数据所使用的总带宽量，但非常精确地显示，没有关于主机连接或进程的任何内部详细信息。\nvnstat更像是一款制作历史报告的工具，显示每天或过去一个月使用了多少带宽。它并不是严格意义上的实时监控网络的工具。\nvnstat支持许多选项，支持哪些选项方面的详细信息请参阅参考手册页。\n安装vnstat\n9. bwm-ng bwm-ng（下一代带宽监控器）是另一款非常简单的实时网络负载监控工具，可以报告摘要信息，显示进出系统上所有可用网络接口的不同数据的传输速度。\n如果控制台足够大，bwm-ng还能使用curses2输出模式，为流量绘制条形图。\n安装bwm-ng：在CentOS上，可以从Epel来安装bwm-ng。\n10. cbm：Color Bandwidth Meter 这是一款小巧简单的带宽监控工具，可以显示通过诸网络接口的流量大小。没有进一步的选项，仅仅实时显示和更新流量的统计信息。\n11. speedometer 这是另一款小巧而简单的工具，仅仅绘制外观漂亮的图形，显示通过某个接口传输的入站流量和出站流量。\n安装speedometer\n12. pktstat pktstat可以实时显示所有活动连接，并显示哪些数据通过这些活动连接传输的速度。它还可以显示连接类型，比如TCP连接或UDP连接；如果涉及HTTP连接，还会显示关于HTTP请求的详细信息。\n13. netwatch netwatch是netdiag工具库的一部分，它也可以显示本地主机与其他远程主机之间的连接，并显示哪些数据在每个连接上所传输的速度。\n14. trafshow 与netwatch和pktstat一样，trafshow也可以报告当前活动连接、它们使用的协议以及每条连接上的数据传输速度。它能使用pcap类型过滤器，对连接进行过滤。\n只监控TCP连接\n15. netload netload命令只显示关于当前流量负载的一份简短报告，并显示自程序启动以来所传输的总字节量。没有更多的功能特性。它是netdiag的一部分。\n16. ifstat ifstat能够以批处理式模式显示网络带宽。输出采用的一种格式便于用户使用其他程序或实用工具来记入日志和分析。\n安装ifstat：Ubuntu、Debian和Fedora用户在默认软件库里面就有它。CentOS用户则需要从Repoforge获得它，因为Epel里面没有它。\n17. dstat dstat是一款用途广泛的工具（用python语言编写），它可以监控系统的不同统计信息，并使用批处理模式来报告，或者将相关数据记入到CSV或类似的文件。这个例子显示了如何使用dstat来报告网络带宽。\n安装dstat\n18. collectl collectl以一种类似dstat的格式报告系统的统计信息；与dstat一样，它也收集关于系统不同资源（如处理器、内存和网络等）的统计信息。这里给出的一个简单例子显示了如何使用collectl来报告网络使用/带宽。\n结束语：上述几个使用方便的命令可以迅速检查Linux服务器上的网络带宽使用情况。不过，这些命令需要用户通过SSH登录到远程服务器。另外，基于Web的监控工具也可以用来实现同样的任务。\nntop和darkstat是面向Linux系统的其中两个基本的基于Web的网络监控工具。除此之外还有企业级监控工具，比如nagios，它们提供了一批功能特性，不仅仅可以监控服务器，还能监控整个基础设施。\n","id":0,"section":"posts","summary":"导读 本文介绍了一些可以用来监控网络使用情况的Linux命令行工具。这些工具可以监控通过网络接口传输的数据，并测量目前哪些数据所传输的速度。入","tags":["运维"],"title":"18个分析Linux系统占用网络带宽的程序","uri":"https://www.devopsman.cn/2021/08/13/","year":"2021"},{"content":"Redis 用的好，加薪少不了，面试被问redis是个大概率事件，与其等着面试官问你在哪场景用过，不如给他雷霆一击，有理有据一口气说出16 个常见使用场景，干翻他！\n1、缓存 String类型\n例如：热点数据缓存（例如报表、明星出轨），对象缓存、全页缓存、可以提升热点数据的访问数据。\n2、数据共享分布式 String 类型，因为 Redis 是分布式的独立服务，可以在多个应用之间共享\n例如：分布式Session\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.session\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-session-data-redis\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 3、分布式锁  String 类型setnx方法  只有不存在时才能添加成功，返回true\npublic static boolean getLock(String key) { Long flag = jedis.setnx(key, \u0026#34;1\u0026#34;); if (flag == 1) { jedis.expire(key, 10); } return flag == 1; } public static void releaseLock(String key) { jedis.del(key); } 4、全局ID  int类型，incrby，利用原子性  incrby userid 1000 分库分表的场景，一次性拿一段\n5、计数器  int类型，incr方法  例如：文章的阅读量、微博点赞数、允许一定的延迟，先写入Redis再定时同步到数据库\n6、限流  int类型，incr方法  以访问者的ip和其他信息作为key，访问一次增加一次计数，超过次数则返回false\n7、位统计  String类型的bitcount（1.6.6的bitmap数据结构介绍）  字符是以8位二进制存储的\nset k1 a setbit k1 6 1 setbit k1 7 0 get k1 /* 6 7 代表的a的二进制位的修改 a 对应的ASCII码是97，转换为二进制数据是01100001 b 对应的ASCII码是98，转换为二进制数据是01100010 因为bit非常节省空间（1 MB=8388608 bit），可以用来做大数据量的统计。 */ 例如：在线用户统计，留存用户统计\nsetbit onlineusers 01 setbit onlineusers 11 setbit onlineusers 20 支持按位与、按位或等等操作\nBITOPANDdestkeykey[key...] ，对一个或多个 key 求逻辑并，并将结果保存到 destkey 。 BITOPORdestkeykey[key...] ，对一个或多个 key 求逻辑或，并将结果保存到 destkey 。 BITOPXORdestkeykey[key...] ，对一个或多个 key 求逻辑异或，并将结果保存到 destkey 。 BITOPNOTdestkeykey ，对给定 key 求逻辑非，并将结果保存到 destkey 。 计算出7天都在线的用户\nBITOP \u0026#34;AND\u0026#34; \u0026#34;7_days_both_online_users\u0026#34; \u0026#34;day_1_online_users\u0026#34; \u0026#34;day_2_online_users\u0026#34; ... \u0026#34;day_7_online_users\u0026#34; 8、购物车  String 或hash。所有String可以做的hash都可以做   key：用户id；field：商品id；value：商品数量。 +1：hincr。-1：hdecr。删除：hdel。全选：hgetall。商品数：hlen。  9、用户消息时间线timeline list，双向链表，直接作为timeline就好了。插入有序\n10、消息队列 List提供了两个阻塞的弹出操作：blpop/brpop，可以设置超时时间\n blpop：blpop key1 timeout 移除并获取列表的第一个元素，如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止。 brpop：brpop key1 timeout 移除并获取列表的最后一个元素，如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止。  上面的操作。其实就是java的阻塞队列。学习的东西越多。学习成本越低\n 队列：先进先除：rpush blpop，左头右尾，右边进入队列，左边出队列 栈：先进后出：rpush brpop  11、抽奖 自带一个随机获得值\nspop myset 12、点赞、签到、打卡 假如上面的微博ID是t1001，用户ID是u3001\n用 like:t1001 来维护 t1001 这条微博的所有点赞用户\n 点赞了这条微博：sadd like:t1001 u3001 取消点赞：srem like:t1001 u3001 是否点赞：sismember like:t1001 u3001 点赞的所有用户：smembers like:t1001 点赞数：scard like:t1001  是不是比数据库简单多了。\n13、商品标签 老规矩，用 tags:i5001 来维护商品所有的标签。\n sadd tags:i5001 画面清晰细腻 sadd tags:i5001 真彩清晰显示屏 sadd tags:i5001 流程至极  14、商品筛选 // 获取差集 sdiff set1 set2 // 获取交集（intersection ） sinter set1 set2 // 获取并集 sunion set1 set2 假如：iPhone11 上市了\nsadd brand:apple iPhone11 sadd brand:ios iPhone11 sad screensize:6.0-6.24 iPhone11 sad screentype:lcd iPhone 11 赛选商品，苹果的、ios的、屏幕在6.0-6.24之间的，屏幕材质是LCD屏幕\nsinter brand:apple brand:ios screensize:6.0-6.24 screentype:lcd 15、用户关注、推荐模型 follow 关注 fans 粉丝\n相互关注：\n sadd 1:follow 2 sadd 2:fans 1 sadd 1:fans 2 sadd 2:follow 1  我关注的人也关注了他(取交集)：\n sinter 1:follow 2:fans  可能认识的人：\n 用户1可能认识的人(差集)：sdiff 2:follow 1:follow 用户2可能认识的人：sdiff 1:follow 2:follow  16、排行榜 id 为6001 的新闻点击数加1：\nzincrby hotNews:20190926 1 n6001\r获取今天点击最多的15条：\nzrevrange hotNews:20190926 0 15 withscores\rRedis 用的好，加薪少不了。\n","id":1,"section":"posts","summary":"Redis 用的好，加薪少不了，面试被问redis是个大概率事件，与其等着面试官问你在哪场景用过，不如给他雷霆一击，有理有据一口气说出16 个常见使用场","tags":["Redis","运维"],"title":"好家伙！原来Redis是这样用的（16个电商应用场景）","uri":"https://www.devopsman.cn/2021/08/12/","year":"2021"},{"content":"为了帮助系统管理员更好地保护开源容器编排工具Kubernetes，美国国家安全局(National Security Agency)和网络安全和基础设施安全局(Cybersecurity and Infrastructure Security Agency)发布了一份新报告，Kubernetes加固指南详细描述了Kubernetes环境面临的威胁，并提供了配置指南以最小化风险。\n据美国国家安全局官员称，Kubernetes集群通常托管在云环境中，与传统软件平台相比，它提供了更大的灵活性，但也经常成为攻击者的目标，这些攻击者试图窃取数据或计算机能力，用于加密货币挖掘或进行拒绝服务攻击。与大多数系统一样，它们很容易受到供应链黑客、恶意威胁行为者和内部威胁的攻击。\n该报告建议对Kubernetes系统进行加固，方法是扫描容器和pod，查找漏洞或错误配置，以尽可能低的权限运行容器和pod，并使用网络隔离、防火墙、强身份验证和日志审计。\n虽然该指导方针的目标是国家安全系统和关键基础设施组织的管理者，但也鼓励联邦和州、地方、部落和地区政府网络的管理者实施所提供的建议。\n报告认为，Kubernetes中三种常见的妥协来源是供应链风险、恶意威胁行为者和内部威胁。供应链风险通常难以减轻，并且可能在容器构建周期或基础设施获取过程中产生。恶意威胁行为者可以利用Kubernetes体系结构组件(如控制平面、工作节点或容器化应用程序)中的漏洞和错误配置。内部威胁可以是管理员、用户或云服务提供商。对组织的Kubernetes基础设施有特殊访问权限的内部人员可能会滥用这些特权。\n指南描述了与设置和保护Kubernetes集群相关的安全挑战。它包括加固策略，以避免常见的错误配置，并指导国家安全系统的系统管理员和开发人员如何部署Kubernetes，以及推荐的加固措施和缓解措施的示例配置。指南详细说明了以下缓解措施:\n 扫描容器和pods是否存在漏洞或配置错误。 以尽可能少的权限运行容器和pods。 使用网络隔离来控制可能造成的损害。 使用防火墙限制不必要的网络连接，并使用加密保护机密性。 使用强认证和授权，限制用户和管理员的访问权限，限制攻击面。 使用日志审计，以便管理员可以监视活动并向其发出警报，报告潜在的恶意行为。 定期检查所有Kubernetes设置，并使用漏洞扫描来帮助管理员。 确保适当地考虑风险并应用安全补丁。  更多的安全加固指导，请参阅互联网安全中心Kubernetes基准测试、Docker和Kubernetes安全技术实施指南、网络安全和基础设施安全局(CISA)分析报告和Kubernetes文档。可在公众号对话框回复关键字：「nsa」免费获取。\n","id":2,"section":"posts","summary":"为了帮助系统管理员更好地保护开源容器编排工具Kubernetes，美国国家安全局(National Security Agency)和网络安全和基础设施安全局","tags":["安全","Kubernetes"],"title":"美国安全局 NSA、CISA 发布 Kubernetes 安全加固指南","uri":"https://www.devopsman.cn/2021/08/11/","year":"2021"},{"content":"对于刚接触容器的人来说，他们很容易被自己构建的 Docker 镜像体积吓到，我只需要一个几 MB 的可执行文件而已，为何镜像的体积会达到 1 GB 以上？本文将会介绍几个奇技淫巧来帮助你精简镜像，同时又不牺牲开发人员和运维人员的操作便利性。本系列文章将分为三个部分：\n第一部分着重介绍多阶段构建（multi-stage builds），因为这是镜像精简之路至关重要的一环。在这部分内容中，我会解释静态链接和动态链接的区别，它们对镜像带来的影响，以及如何避免那些不好的影响。中间会穿插一部分对 Alpine 镜像的介绍。\n第二部分将会针对不同的语言来选择适当的精简策略，其中主要讨论 Go，同时也涉及到了 Java，Node，Python，Ruby 和 Rust。这一部分也会详细介绍 Alpine 镜像的避坑指南。什么？你不知道 Alpine 镜像有哪些坑？我来告诉你。\n第三部分将会探讨适用于大多数语言和框架的通用精简策略，例如使用常见的基础镜像、提取可执行文件和减小每一层的体积。同时还会介绍一些更加奇特或激进的工具，例如 Bazel，Distroless，DockerSlim 和 UPX，虽然这些工具在某些特定场景下能带来奇效，但大多情况下会起到反作用。\n本文介绍第一部分。\n万恶之源 我敢打赌，每一个初次使用自己写好的代码构建 Docker 镜像的人都会被镜像的体积吓到，来看一个例子。\n让我们搬出那个屡试不爽的 hello world C 程序：\n/* hello.c */int main () { puts(\u0026#34;Hello, world!\u0026#34;); return0;} 并通过下面的 Dockerfile 构建镜像：\nFROM gccCOPY hello.c .RUN gcc -o hello hello.cCMD [\u0026#34;./hello\u0026#34;] 然后你会发现构建成功的镜像体积远远超过了 1 GB……因为该镜像包含了整个 gcc 镜像的内容。\n如果使用 Ubuntu 镜像，安装 C 编译器，最后编译程序，你会得到一个大概 300 MB 大小的镜像，比上面的镜像小多了。但还是不够小，因为编译好的可执行文件还不到 20 KB：\n$ ls -l hello-rwxr-xr-x 1 root root 16384 Nov 18 14:36 hello 类似地，Go 语言版本的 hello world 会得到相同的结果：\npackage main import\u0026#34;fmt\u0026#34; func main () { fmt.Println(\u0026#34;Hello, world!\u0026#34;)} 使用基础镜像 golang 构建的镜像大小是 800 MB，而编译后的可执行文件只有 2 MB 大小：\n$ ls -l hello-rwxr-xr-x 1 root root 2008801 Jan 15 16:41 hello 还是不太理想，有没有办法大幅度减少镜像的体积呢？往下看。\n为了更直观地对比不同镜像的大小，所有镜像都使用相同的镜像名，不同的标签。例如：hello:gcc，hello:ubuntu，hello:thisweirdtrick 等等，这样就可以直接使用命令 docker images hello 列出所有镜像名为 hello 的镜像，不会被其他镜像所干扰。\n多阶段构建 要想大幅度减少镜像的体积，多阶段构建是必不可少的。多阶段构建的想法很简单：“我不想在最终的镜像中包含一堆 C 或 Go 编译器和整个编译工具链，我只要一个编译好的可执行文件！”\n多阶段构建可以由多个 FROM 指令识别，每一个 FROM 语句表示一个新的构建阶段，阶段名称可以用 AS 参数指定，例如：\nFROMgcc AS mybuildstageCOPY hello.c .RUN gcc -o hello hello.cFROMubuntuCOPY --from=mybuildstage hello .CMD [\u0026#34;./hello\u0026#34;]本例使用基础镜像 gcc 来编译程序 hello.c，然后启动一个新的构建阶段，它以 ubuntu 作为基础镜像，将可执行文件 hello 从上一阶段拷贝到最终的镜像中。最终的镜像大小是 64 MB，比之前的 1.1 GB 减少了 95%：\n$ docker images minimage REPOSITORY TAG ... SIZE minimage hello-c.gcc ... 1.14GB minimage hello-c.gcc.ubuntu ... 64.2MB 还能不能继续优化？当然能。在继续优化之前，先提醒一下：\n在声明构建阶段时，可以不必使用关键词 AS，最终阶段拷贝文件时可以直接使用序号表示之前的构建阶段（从零开始）。也就是说，下面两行是等效的：\nCOPY --from=mybuildstage hello .COPY --from=0 hello .如果 Dockerfile 内容不是很复杂，构建阶段也不是很多，可以直接使用序号表示构建阶段。一旦 Dockerfile 变复杂了，构建阶段增多了，最好还是通过关键词 AS 为每个阶段命名，这样也便于后期维护。\n使用经典的基础镜像\n我强烈建议在构建的第一阶段使用经典的基础镜像，这里经典的镜像指的是 CentOS，Debian，Fedora 和 Ubuntu 之类的镜像。你可能还听说过 Alpine 镜像，不要用它！至少暂时不要用，后面我会告诉你有哪些坑。\nCOPY \u0026ndash;from 使用绝对路径\n从上一个构建阶段拷贝文件时，使用的路径是相对于上一阶段的根目录的。如果你使用 golang 镜像作为构建阶段的基础镜像，就会遇到类似的问题。假设使用下面的 Dockerfile 来构建镜像：\nFROMgolangCOPY hello.go .RUN go build hello.goFROMubuntuCOPY --from=0 hello .CMD [\u0026#34;./hello\u0026#34;]你会看到这样的报错：\nCOPY failed: stat /var/lib/docker/overlay2/1be...868/merged/hello: no such file or directory 这是因为 COPY 命令想要拷贝的是 /hello，而 golang 镜像的 WORKDIR 是 /go，所以可执行文件的真正路径是 /go/hello。\n当然你可以使用绝对路径来解决这个问题，但如果后面基础镜像改变了 WORKDIR 怎么办？你还得不断地修改绝对路径，所以这个方案还是不太优雅。最好的方法是在第一阶段指定 WORKDIR，在第二阶段使用绝对路径拷贝文件，这样即使基础镜像修改了 WORKDIR，也不会影响到镜像的构建。例如：\nFROM golangWORKDIR /srcCOPY hello.go .RUN go build hello.goFROM ubuntuCOPY --from=0 /src/hello .CMD [\u0026quot;./hello\u0026quot;] 最后的效果还是很惊人的，将镜像的体积直接从 800 MB 降低到了 66 MB：\n$ docker images minimageREPOSITORY TAG ... SIZE minimage hello-go.golang ... 805MB minimage hello-go.golang.ubuntu-workdir ... 66.2MB FROM scratch 的魔力 回到我们的 hello world，C 语言版本的程序大小为 16 kB，Go 语言版本的程序大小为 2 MB，那么我们到底能不能将镜像缩减到这么小？能否构建一个只包含我需要的程序，没有任何多余文件的镜像？\n答案是肯定的，你只需要将多阶段构建的第二阶段的基础镜像改为 scratch 就好了。scratch 是一个虚拟镜像，不能被 pull，也不能运行，因为它表示空、nothing！这就意味着新镜像的构建是从零开始，不存在其他的镜像层。例如：\nFROMgolangCOPY hello.go .RUN go build hello.goFROMscratchCOPY --from=0 /go/hello .CMD [\u0026#34;./hello\u0026#34;]这一次构建的镜像大小正好就是 2 MB，堪称完美！\n然而，但是，使用 scratch 作为基础镜像时会带来很多的不便，且听我一一道来。\n缺少 shell\nscratch 镜像的第一个不便是没有 shell，这就意味着 CMD/RUN 语句中不能使用字符串，例如：\n... FROM scratch COPY --from=0 /go/hello . CMD ./hello 如果你使用构建好的镜像创建并运行容器，就会遇到下面的报错：\ndocker: Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused \u0026quot;exec: \\\u0026quot;/bin/sh\\\u0026quot;: stat /bin/sh: no such file or directory\u0026quot;: unknown. 从报错信息可以看出，镜像中并不包含 /bin/sh，所以无法运行程序。这是因为当你在 CMD/RUN 语句中使用字符串作为参数时，这些参数会被放到 /bin/sh 中执行，也就是说，下面这两条语句是等效的：\nCMD ./helloCMD /bin/sh -c \u0026quot;./hello\u0026quot; 解决办法其实也很简单：**使用 JSON 语法取代字符串语法。**例如，将 CMD ./hello 替换为 CMD [\u0026rdquo;./hello\u0026rdquo;]，这样 Docker 就会直接运行程序，不会把它放到 shell 中运行。\n缺少调试工具\nscratch 镜像不包含任何调试工具，ls、ps、ping 这些统统没有，当然了，shell 也没有（上文提过了），你无法使用 docker exec 进入容器，也无法查看网络堆栈信息等等。\n如果想查看容器中的文件，可以使用 docker cp；如果想查看或调试网络堆栈，可以使用 docker run \u0026ndash;net container:，或者使用 nsenter；为了更好地调试容器，Kubernetes 也引入了一个新概念叫 Ephemeral Containers[1]，但现在还是 Alpha 特性。\n虽然有这么多杂七杂八的方法可以帮助我们调试容器，但它们会将事情变得更加复杂，我们追求的是简单，越简单越好。\n折中一下可以选择 busybox 或 alpine 镜像来替代 scratch，虽然它们多了那么几 MB，但从整体来看，这只是牺牲了少量的空间来换取调试的便利性，还是很值得的。\n缺少 libc\n这是最难解决的问题。使用 scratch 作为基础镜像时，Go 语言版本的 hello world 跑得很欢快，C 语言版本就不行了，或者换个更复杂的 Go 程序也是跑不起来的（例如用到了网络相关的工具包），你会遇到类似于下面的错误：\nstandard_init_linux.go:211: exec user process caused \u0026quot;no such file or directory\u0026quot; 从报错信息可以看出缺少文件，但没有告诉我们到底缺少哪些文件，其实这些文件就是程序运行所必需的动态库（dynamic library）。\n那么，什么是动态库？为什么需要动态库？\n所谓动态库、静态库，指的是程序编译的链接阶段，链接成可执行文件的方式。静态库指的是在链接阶段将汇编生成的目标文件.o 与引用到的库一起链接打包到可执行文件中，因此对应的链接方式称为静态链接（static linking）。而动态库在程序编译时并不会被连接到目标代码中，而是在程序运行是才被载入，因此对应的链接方式称为动态链接（dynamic linking）。\n90 年代的程序大多使用的是静态链接，因为当时的程序大多数都运行在软盘或者盒式磁带上，而且当时根本不存在标准库。这样程序在运行时与函数库再无瓜葛，移植方便。但对于 Linux 这样的分时系统，会在在同一块硬盘上并发运行多个程序，这些程序基本上都会用到标准的 C 库，这时使用动态链接的优点就体现出来了。使用动态链接时，可执行文件不包含标准库文件，只包含到这些库文件的索引。例如，某程序依赖于库文件 libtrigonometry.so 中的 cos 和 sin 函数，该程序运行时就会根据索引找到并加载 libtrigonometry.so，然后程序就可以调用这个库文件中的函数。\n使用动态链接的好处显而易见：\n 节省磁盘空间，不同的程序可以共享常见的库。 节省内存，共享的库只需从磁盘中加载到内存一次，然后在不同的程序之间共享。 更便于维护，库文件更新后，不需要重新编译使用该库的所有程序。严格来说，动态库与共享库（shared libraries）相结合才能达到节省内存的功效。Linux 中动态库的扩展名是 .so（ shared object），而 Windows 中动态库的扩展名是 .DLL（Dynamic-link library[2]）。  回到最初的问题，默认情况下，C 程序使用的是动态链接，Go 程序也是。上面的 hello world 程序使用了标准库文件 libc.so.6，所以只有镜像中包含该文件，程序才能正常运行。使用 scratch 作为基础镜像肯定是不行的，使用 busybox 和 alpine 也不行，因为 busybox 不包含标准库，而 alpine 使用的标准库是 musl libc，与大家常用的标准库 glibc 不兼容，后续的文章会详细解读，这里就不赘述了。\n那么该如何解决标准库的问题呢？有三种方案。\n1、使用静态库\n我们可以让编译器使用静态库编译程序，办法有很多，如果使用 gcc 作为编译器，只需加上一个参数 -static：\n$ gcc -o hello hello.c -static 编译完的可执行文件大小为 760 kB，相比于之前的 16kB 是大了好多，这是因为可执行文件中包含了其运行所需要的库文件。编译完的程序就可以跑在 scratch 镜像中了。\n如果使用 alpine 镜像作为基础镜像来编译，得到的可执行文件会更小（\u0026lt; 100kB），下篇文章会详述。\n2、拷贝库文件到镜像中\n为了找出程序运行需要哪些库文件，可以使用 ldd 工具：\n$ ldd hello linux-vdso.so.1 (0x00007ffdf8acb000) libc.so.6 =\u0026gt; /usr/lib/libc.so.6 (0x00007ff897ef6000) /lib64/ld-linux-x86-64.so.2 =\u0026gt; /usr/lib64/ld-linux-x86-64.so.2 (0x00007ff8980f7000) 从输出结果可知，该程序只需要 libc.so.6 这一个库文件。linux-vdso.so.1 与一种叫做 VDSO[3] 的机制有关，用来加速某些系统调用，可有可无。ld-linux-x86-64.so.2 表示动态链接器本身，包含了所有依赖的库文件的信息。\n你可以选择将 ldd 列出的所有库文件拷贝到镜像中，但这会很难维护，特别是当程序有大量依赖库时。对于 hello world 程序来说，拷贝库文件完全没有问题，但对于更复杂的程序（例如使用到 DNS 的程序），就会遇到令人费解的问题：glibc（GNU C library）通过一种相当复杂的机制来实现 DNS，这种机制叫 NSS（Name Service Switch, 名称服务开关）。它需要一个配置文件 /etc/nsswitch.conf 和额外的函数库，但使用 ldd 时不会显示这些函数库，因为这些库在程序运行后才会加载。如果想让 DNS 解析正确工作，必须要拷贝这些额外的库文件（/lib64/libnss_*）。\n我个人不建议直接拷贝库文件，因为它非常难以维护，后期需要不断地更改，而且还有很多未知的隐患。\n3、使用 busybox:glibc 作为基础镜像\n有一个镜像可以完美解决所有的这些问题，那就是 busybox:glibc。它只有 5 MB 大小，并且包含了 glibc 和各种调试工具。如果你想选择一个合适的镜像来运行使用动态链接的程序，busybox:glibc 是最好的选择。\n注意：如果你的程序使用到了除标准库之外的库，仍然需要将这些库文件拷贝到镜像中。\n总结 最后来对比一下不同构建方法构建的镜像大小：\n 原始的构建方法：1.14 GB 使用 ubuntu 镜像的多阶段构建：64.2 MB 使用 alpine 镜像和静态 glibc：6.5 MB 使用 alpine 镜像和动态库：5.6 MB 使用 scratch 镜像和静态 glibc：940 kB 使用 scratch 镜像和静态 musl libc：94 kB  最终我们将镜像的体积减少了 99.99%。\n但我不建议使用 sratch 作为基础镜像，因为调试起来非常麻烦，但如果你喜欢，那我也不会拦着你。\n下篇文章将会着重介绍 Go 语言的镜像精简策略，其中会花很大的篇幅来讨论 alpine 镜像，因为它实在是太酷了，在使用它之前必须得摸清它的底细。\n相关链接：\n https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/ https://en.wikipedia.org/wiki/Dynamic-link_library https://en.wikipedia.org/wiki/VDSO  ","id":3,"section":"posts","summary":"对于刚接触容器的人来说，他们很容易被自己构建的 Docker 镜像体积吓到，我只需要一个几 MB 的可执行文件而已，为何镜像的体积会达到 1 GB 以上？本文将会介绍几","tags":["Docker","镜像优化"],"title":"我是如何将 Docker 镜像体积减小 90%","uri":"https://www.devopsman.cn/2021/08/04/","year":"2021"},{"content":"Prometheus已成为cloud-native世界中的默认监控应用程序和系统。对于真是使用案例，Prometheus应该是高可用的，这是有挑战的。一旦在高可用性模式下运行Prometheus，就会遇到很多问题，例如数据重复，为重复数据实现single pane等。为了解决此问题，Cortex诞生了。Cortex是一个CNCF sandbox project，旨在为使用Prometheus 收集的指标提供长期存储和全局指标视图。首先让我们看一下Cortex的主要目标，然后看一下它为Prometheus解决的一些问题。\n 水平可伸缩性– Cortex可以分成多个微服务，每个微服务都可以独立地水平伸缩。例如，如果许多Prometheus实例正在向Cortex发送数据，则可以扩展Ingester微服务。如果cortex有许多的查询，则可以扩展Querier或Query Frontend微服务。 高可用性– Cortex可以在实例之间复制数据replicate data。这样可以防止数据丢失，并避免度量标准数据出现间断，即使发生机器故障and/orpod被驱逐。 多租户–多个不受信任的parties可以共享同一群集。Cortex在从ingester到querying的整个生命周期中提供数据隔离。这对于为多个单元或应用程序存储数据的大型组织或运行SaaS服务的人员非常有用。 长期存储– Cortex将数据分块存储并为其生成索引。可以将Cortex配置为将其存储在自托管或云提供商支持的数据库或对象存储中。  Cortex的需求 Prometheus高可用性和数据去重 Prometheus默认情况下不具有高可用性。使Prometheus高可用的一种方式是运行多个实例去scraping相同的作业。这些实例在抓取指标时会因微小的时间间隔差异而在数据中产生细微差异。此外，如果其中一个实例宕机了几个小时，那么当查询转发到该实例时，将会出现数据空白。如果我们使用grafana这样的工具将指标展示为图形，我们可能会得到不同的样本值或有数据缺失的图形。\n可以将Cortex配置为从多个HA Prometheus实例读取数据。它从一个主实例main接受指标，并从其他实例放弃该度量。一旦这个副本掉线，Cortex会无缝切换到另一副本并将其标记为主副本main。为此，Cortex着眼于两个标签，一个共同的标签与一个集群（或一组Prometheus）相关联，另一个识别副本。\n全局指标视图 可以将Prometheus实例配置为对cortex执行远程写入。使用此功能，指标可以从多个集群聚合到一个运行cortes的集群中。这为我们提供了一个中心位置，在这里我们可以观察整个基础设施的指标。Cortex提供了与Prometheus/PromQL兼容的端点endpoint。可以将此端点endpoint作为数据源提供给grafana，并与普通Prometheus实例相同的方式执行查询。\n长期储存 普罗米修斯的本地存储不是持久的长期存储。发送到cortex的指标被存储在已配置的存储服务中。如果使用云存储，这将使您从运行自己的数据库的麻烦中解脱出来。你还可以享受云提供商提供的SLA。Cortex还支持用于存储块的对象存储：\n GCS S3  多租户 当向cortex写入指标时，通过设置http头(X-Scope-OrgID)来提供多租户。查询时必须提供相同的值。在下面的例子中，这个headers是使用nginx反向代理设置的。\n架构图 Cortex架构（源）\n Nginx/gateway–一个位于cortex前面的反向代理，将收到的所有请求转发给相应的服务。 分发服务器Distributor–处理传入的指标，将其拆分为多个批次，然后将其传递给Ingesters。如果复制因子replication factor设置为\u0026gt; 1，则数据将发送到多个实例。 接收器Ingester–此服务负责将数据写入已配置的存储后端。Ingester是半状态的，因为它们保留了最后12个小时的样本。这些样本将被批处理并压缩，然后再写入块存储。 查询前端Query Frontend–一个可选组件，用于对查询请求进行排队，并在失败时重试它们。结果也被缓存以提高性能 查询器Querier–查询器处理PromQL的求值。如果是最近的数据，则从大块存储和或内部获取样本  其他组件：\n Ruler–处理alertmanager产生的警报 Alertmanager –评估警报规则 ConfigsAPI –在Postgres中存储Ruler和Alertmanager的配置 Table Manager–负责在选定的块chunk/索引index存储后端中创建表 Consul –存储分发服务器distributor生成的一致的哈希环(hash ring)。分发服务器在发送指标时使用散列值来选择ingester。  与其他选项的异同 Thanos Thanos和Cortex具有非常相似的目标：聚合指标，将其存储在块存储中，并为所有度量使用一块single pane。因此，两个项目重用大量Prometheus代码也就不足为奇了。但是，有一些关键差异可能会帮助您决定使用哪个。\n   Thanos Cortex     最近的数据存储在Prometheus中 最近的数据存储在Ingesters中（Cortex组件）   使用可以将数据写入块存储的Sidecar 通过prometheus的远程写将数据发送到cortex   单租户 多租户   手动分片 根据标签自动分片数据   Prom TSDB块 索引块 index   下采样：历史数据可以汇总（例如，将5秒的样本平均为1分钟的样本） 查询分片（将30天转换为30天的一天查询）   需要Ingress到运行Prometheus的集群中进行查询 运行Prometheus的集群仅需要出口Egress    演练 让我们通过安装一个真实的示例并通过多个Prometheus和Grafana对其进行配置以可视化数据来试用Cortex。\ngit clone https://github.com/kanuahs/cortex-demo.git cd cortex-demo Prometheus and Cortex with Docker Compose 为了简单设置，我们将使用docker-compose启动以下服务：\n 三个Prometheus容器 Consul 三个Cortex容器 Grafana  为了简单起见，我们将使用多功能的cortex配置。这将cortex作为一个独立的应用程序运行。我们将运行它的三个实例来检查复制。有三个Prometheus配置文件。它们具有外部标签，在执行远程写入时将标签添加到所有指标。Prometheus1和Prometheus3容器写入Cortex1，而Prometheus2容器写入Cortex2。我们将在Cortex3上运行查询。以下代码片段显示了三个Prometheus实例的配置差异。\n# Prometheus one global: # ... external_labels: cluster: two # ... remote_write: - url: http://cortex2:9009/api/prom/push # Prometheus two global: # ... external_labels: cluster: two # ... remote_write: - url: http://cortex1:9009/api/prom/push # Prometheus three global: # ... external_labels: cluster: tree # ... remote_write: - url: http://cortex1:9009/api/prom/push 使用docker-compose启动所有服务。\ndocker-compose -f docker-demo/docker-compose.yaml up 转到http://localhost:3000/explore，使用凭据admin/admin登录，然后选择cortex3作为数据源。运行示例查询（例如up）Cortex将返回所有3个Prometheus容器的指标：完成后，运行以下命令进行清理\ndocker-compose -f docker-demo/docker-compose.yaml down 在Kubernetes中部署Cortex以及Prometheus的依赖 如架构所示，我们将Cortex部署为一组微服务。我们还需要Helm来部署依赖项（Cassandra）和其他服务（Grafana，Prometheus）。如果尚未安装Helm，则可以按照Helm文档中的快速入门指南进行操作。让我们开始吧，首先我们将部署cortex组件。\nkubectl apply -f k8s/ 我们还将使用Helm安装Prometheus。下面的命令将为我们完成这两项工作：\n 创建一个名为\u0026rdquo; cluster\u0026quot;的外部标签，并将该标签的值设置为\u0026quot;one\u0026rdquo;。这将有助于分离不同的Prometheus实例 设置cortex的远程写入。  helm install stable/prometheus \\ --name prom-one \\ --set server.global.external_labels.cluster=one \\ --set serverFiles.\u0026quot;prometheus\\.yml\u0026quot;.remote_write[0].url=http://nginx.default.svc.cluster.local:80/api/prom/push 现在，我们将部署Grafana。下面的命令使用Grafana的配置功能在Pod启动时将Cortex添加为数据源。\nhelm install stable/grafana --name=grafana \\ --set datasources.\u0026quot;datasources\\.yaml\u0026quot;.apiVersion=1 \\ --set datasources.\u0026quot;datasources\\.yaml\u0026quot;.datasources[0].name=cortex \\ --set datasources.\u0026quot;datasources\\.yaml\u0026quot;.datasources[0].type=prometheus \\ --set datasources.\u0026quot;datasources\\.yaml\u0026quot;.datasources[0].url=http://nginx.default.svc.cluster.local/api/prom \\ --set datasources.\u0026quot;datasources\\.yaml\u0026quot;.datasources[0].access=proxy \\ --set datasources.\u0026quot;datasources\\.yaml\u0026quot;.datasources[0].isDefault=true kubectl port-forward svc/grafana 3000:80 运行以下命令以获取Grafana管理员密码。之后，打开localhost:3000/explore并使用用户名\u0026quot;admin\u0026quot;和打印的密码登录。\nkubectl get secret --namespace default grafana -o jsonpath=\u0026quot;{.data.admin-password}\u0026quot; | base64 --decode ; echo 以下是一些示例查询，您可以运行这些查询来测试Prometheus是否向Cortex发送指标：\nup{cluster=\u0026quot;one\u0026quot;} prometheus_tsdb_head_samples_appended_total{cluster=\u0026quot;one\u0026quot;} Cleanup\nkubectl delete -f k8s/ helm delete --purge prom-one helm delete --purge grafana ingester pods将卡在终止阶段。这是设计使然的，因为ingester是半状态的，并且将在终止之前尝试将其数据刷新到其他ingester。这使得升级和回滚成为可能，同时避免了数据丢失。在这种情况下，我们只是尝试一下，而不关心数据，因此我们可以使用以下命令强制将其删除：\nkubectl delete pod -l name=ingester --grace-period=0 --force 具有数据去重功能的HA Prometheus设置 此设置与上一个非常相似。主要区别在于我们正在部署两个Prometheus实例。两者都具有被设置为相同值\u0026quot;one\u0026quot;的集群标签和唯一的副本标签。分发器组件已配置为基于这两个标签执行重复数据删除。如果Cortex在当前规定的时间（超过30秒）内没有从当前副本接收指标，它将故障转移到下一个发送样本的副本。\nkubectl apply -f k8s-ha/ helm install stable/prometheus \\ --name prom-one \\ --set server.global.external_labels.cluster=one \\ --set server.global.external_labels.replica=one \\ --set serverFiles.\u0026quot;prometheus\\.yml\u0026quot;.remote_write[0].url=http://nginx.default.svc.cluster.local:80/api/prom/push helm install stable/prometheus \\ --name prom-two \\ --set server.global.external_labels.cluster=one \\ --set server.global.external_labels.replica=two \\ --set serverFiles.\u0026quot;prometheus\\.yml\u0026quot;.remote_write[0].url=http://nginx.default.svc.cluster.local:80/api/prom/push helm install stable/grafana --name=grafana \\ --set datasources.\u0026quot;datasources\\.yaml\u0026quot;.apiVersion=1 \\ --set datasources.\u0026quot;datasources\\.yaml\u0026quot;.datasources[0].name=cortex \\ --set datasources.\u0026quot;datasources\\.yaml\u0026quot;.datasources[0].type=prometheus \\ --set datasources.\u0026quot;datasources\\.yaml\u0026quot;.datasources[0].url=http://nginx.default.svc.cluster.local/api/prom \\ --set datasources.\u0026quot;datasources\\.yaml\u0026quot;.datasources[0].access=proxy \\ --set datasources.\u0026quot;datasources\\.yaml\u0026quot;.datasources[0].isDefault=true kubectl port-forward svc/grafana 3000:80 运行以下命令以获取grafana管理员密码。之后，打开localhost:3000/explore并使用用户名\u0026quot;admin\u0026quot;和打印的密码登录。\nkubectl get secret --namespace default grafana -o jsonpath=\u0026quot;{.data.admin-password}\u0026quot; | base64 --decode ; echo 以下是一些示例查询，您可以运行这些查询来测试Prometheus是否向Cortex发送指标：\nup{cluster=\u0026quot;one\u0026quot;} prometheus_tsdb_head_samples_appended_total{cluster=\u0026quot;one\u0026quot;} 要测试HA，我们可以尝试删除Prometheus窗格之一。\nkubectl delete pod -l app=prometheus,component=server,release=prom-one Grafana图中应该没有数据缺失，因为Cortex将故障转移到另一个实例。\n清理\nkubectl delete -f k8s/ helm delete --purge prom-one helm delete --purge prom-two helm delete --purge grafana 使用Cassandra作为索引和块存储 在前两个示例中，我们使用dynamodb-local作为索引存储，并使用fakes3作为块存储。在此示例中，我们将使用Apache Cassandra进行索引存储和块存储。\n以下命令将启用helm incubator repo，使用helm安装Cassandra，并等待3个副本准备就绪。\nhelm repo add incubator https://kubernetes-charts-incubator.storage.googleapis.com/ helm install --wait --name=cassie incubator/cassandra 准备好Cassandra后，继续安装所有其他服务。\nkubectl apply -f k8s-cassandra/ helm install stable/prometheus \\ --name prom-one \\ --set server.global.external_labels.cluster=one \\ --set serverFiles.\u0026quot;prometheus\\.yml\u0026quot;.remote_write[0].url=http://nginx.default.svc.cluster.local:80/api/prom/push helm install stable/grafana --name=grafana \\ --set datasources.\u0026quot;datasources\\.yaml\u0026quot;.apiVersion=1 \\ --set datasources.\u0026quot;datasources\\.yaml\u0026quot;.datasources[0].name=cortex \\ --set datasources.\u0026quot;datasources\\.yaml\u0026quot;.datasources[0].type=prometheus \\ --set datasources.\u0026quot;datasources\\.yaml\u0026quot;.datasources[0].url=http://nginx.default.svc.cluster.local/api/prom \\ --set datasources.\u0026quot;datasources\\.yaml\u0026quot;.datasources[0].access=proxy \\ --set datasources.\u0026quot;datasources\\.yaml\u0026quot;.datasources[0].isDefault=true kubectl port-forward svc/grafana 3000:80 运行以下命令以获取Grafana管理员密码。之后，打开localhost:3000/explore并使用用户名admin和打印的密码登录。\nkubectl get secret --namespace default grafana -o jsonpath=\u0026quot;{.data.admin-password}\u0026quot; | base64 --decode ; echo 以下是一些示例查询，您可以运行这些查询来测试Prometheus是否向Cortex发送指标：\nup{cluster=\u0026quot;one\u0026quot;} prometheus_tsdb_head_samples_appended_total{cluster=\u0026quot;one\u0026quot;} 清理\nkubectl delete -f k8s/ helm delete --purge cassie helm delete --purge prom-one helm delete --purge grafana 结论 Cortex是一个强大的工具，可无缝地运行多个Prometheus服务器，同时简化了用户的操作和使用。虽然Thanos确实提供了非常相似的功能，但它们的实现方式却大不相同。通过尝试实现的用例将推动选择Cortex与Thanos。但是，Cortex确实使运行高度可扩展且具有弹性的基于Prometheus的监视系统变得容易。\n","id":4,"section":"posts","summary":"Prometheus已成为cloud-native世界中的默认监控应用程序和系统。对于真是使用案例，Prometheus应该是高可用的，这是","tags":["监控","云原生"],"title":"高可用监控方案Cortex探险","uri":"https://www.devopsman.cn/2021/05/13/","year":"2021"},{"content":"Yaml基础语法与技巧  字符支持：YAML支持Unicode字符集，可以使用UTF-8、UTF-16、UTF-32字符集  date: 2020-06-28 info: - name: Marionxue tags: - num: 1 - descript: \u0026#34;writing somthing here\u0026#34;　 缩进：YAML中缩进是非常重要的规范，不支持Tab，支持空格，没有严格要求空格个数，但是需要确保同一层次的左侧对齐 单行注释：单行注释使用#进行标记，可以在单行的任何位置开始注释的内容 多行注释：不提供特殊的多行注释，使用多行行首的单行注释#实现多行注释的需求  info: # 下面是两个空格，然后一个短横线 - name: Marionxue  基本数据类型：支持整型、浮点型、时间戳类型、Null等基本数据类型 组合数据类型：支持键/值方式和列表类型，并可进行嵌套组合 键/值方式：使用冒号:进行分隔，也可使用{}结合逗号进行表达 列表类型：使用横线-进行分隔，也可使用[]结合逗号进行表达 开始符号: ---用于表示开始的符号，在一个文件中包含多个YAML设定的时候使用非常常见。 结束符号：…用于表示yaml文件结束  --- # start - [blue, red, green] #list - [Age, Bag] - site: {osc:www.oschina.net, baidu: www.baidu.com} # key/value list - describle: | Hi,all: my name is xxx. - code: \u0026gt; fmt.Println(\u0026#34;姓名: %s\u0026#34;, name)   单引号与双引号：字符串类型可以不使用单引号和双引号，使用单引号和双引号与不使用的时候在特殊字符及其转义的时候有些细微的区别，可用倒斜线**（\\）**进行特殊字符转义\n  区块的字串用缩排和修饰词（非必要）来和其他资料分隔，有新行保留（使用符号|）或新行折叠（使用符号\u0026gt;）两种方式\n |表示保留区块中的回车换行 \u0026gt;表示将区块中的回车换行替换为空行，最终连成一行    强制类型转换：可以使用!!用于强制类型转换\n  重复性内容：可以使用锚点标记\u0026amp;和应用标记*结合使用可以处理重复性的内容\n  保留字符：@和`为当前YAML规格的保留字符\n  较长的描绘性说明：使用|与\u0026gt;以及\u0026gt;-来处理常见的对于较长的描绘性说明的要求\n  空白字符限制：在使用逗号及冒号时，后须接一个空白字符\n   YAML使用可打印的Unicode字符，可使用UTF-8或UTF-16 使用空白字符**（不能使用Tab）**分层，同层元素左侧对齐 单行注解由井字号**（ # ）**开始，可以出现在行中任何位置 每个清单成员以单行表示，并用短杠+空白**（- ）**起始 每个杂凑表的成员用冒号+空白**（: ）**分开键和值 杂凑表的键值可以用问号 **(?)**起始，表示多个词汇组成的键值 字串一般不使用引号，但必要的时候可以用引号框住 使用双引号表示字串时，可用倒斜线**（\\）**进行特殊字符转义 区块的字串用缩排和修饰词（非必要）来和其他资料分隔，有新行保留（使用符号|）或新行折叠（使用符号\u0026gt;）两种方式 在单一档案中，可用连续三个连字号（\u0026mdash;）区分多个档案 可选择性的连续三个点号（\u0026hellip;）用来表示档案结尾(在流式传输时非常有用，不需要关闭流即可知道到达结尾处) 重复的内容可使从参考标记星号 (*)复制到锚点标记（\u0026amp;） 指定格式可以使用两个惊叹号 ( !! )，后面接上名称  ","id":5,"section":"posts","summary":"Yaml基础语法与技巧 字符支持：YAML支持Unicode字符集，可以使用UTF-8、UTF-16、UTF-32字符集 date: 2020-06-28 info: - name: Marionxue tags: - num: 1 -","tags":["YAML","云原生"],"title":"YAML工程师是怎么炼成的？","uri":"https://www.devopsman.cn/2020/12/31/","year":"2020"},{"content":"2020年6月24日，BFE开源项目被CNCF （Cloud Native Computing Foundation，云原生计算基金会）正式接纳为Sandbox Project。这是百度第一个被CNCF接纳的开源项目，也是在网络方向上中国第一个被CNCF接纳的开源项目。\nBFE原名为Baidu Front End（百度统一前端），是百度的统一七层流量转发平台。BFE平台目前已接入百度大部分流量，每日转发请求接近1万亿，峰值QPS超过1000万。在2019年百度春晚红包活动中，BFE平台在超大用户压力、数次流量波峰下平稳运行，保证了春晚红包活动的顺利进行。\n作为综合的流量转发平台，BFE平台集成了以下4大功能：\n 流量接入和转发：支持HTTP、HTTPS、HTTP/2、QUIC等多种协议，并支持强大的应用层路由能力 流量全局调度：支持由外网流量调度和内网流量调度共同构成的全局流量调度系统 安全和防攻击：支持黑名单封禁、精细限流和应用层防火墙（WAF）等多种防攻击能力 实时数据分析：支持分钟级的超高维度时序报表  作为BFE平台的核心组件，BFE转发引擎从2012年开始研发，并于2014年使用Go语言完成重构。由于基于Go语言，和业界普遍使用的Nginx开源软件相比，BFE具有以下优势：\n 研发效率高：Go语言的开发效率远高于C语言（及Lua），在代码的可维护性方面也有巨大优势。 系统的安全和稳定性高：Go语言没有C语言固有的缓冲区溢出隐患，规避了大量的稳定性和安全风险；另外对于异常可以捕捉，保证程序在快速迭代上线的情况下也不崩溃。  有理由相信，从长期趋势看，基于更高级编程语言的软件系统会逐步取得竞争的优势。\nCPU等硬件资源的价格仍会快速下降，而开发人力成本、项目研发风险、系统稳定性/安全性方面会成为更重要的决策考虑。从这方面出发，主要基于C语言的Nginx会逐步衰落，而类似BFE这样的基于更高级编程语言的软件会逐步成为主流。\n另外，BFE在设计中，还特别增加了企业级应用场景的考虑：\n 转发场景的直接支持：和Nginx这样从Web Server转型为Proxy的进化路径不同，BFE直接为转发场景设计，从转发模型和转发配置方面更满足转发场景的需求 多租户的支持：在云计算的场景下，多租户复用是普遍的需求。在BFE的设计中，内置提供了多租户的支持。 结构化的配置：BFE的配置设计，大量使用JSON这样的结构化方式，便于和相关配置管理系统对接 丰富的监控探针：作为一个工业级软件，在BFE的设计中充分考虑了线上监控的需求，BFE程序通过HTTP方式向外暴露数千个内部状态变量  为了促进负载均衡技术的交流和发展，BFE的转发引擎于2019年7月正式开源，并获得了广泛的关注。2019年11月19日，BFE开源项目登上GitHub Trending Top 3。2019年12月，BFE开源项目的Github stars超过3000。\nBFE开源支持以下重要能力：\n1、主流网络协议接入\n 支持HTTP/HTTPS/SPDY/HTTP2/WebSocket等 支持TLS/HTTP/ WebSocket反向代理模式  2、可扩展插件框架\n 通过可扩展插件框架，快速定制开发扩展模块，满足业务定制化需求 内置重写、重定向、流量修改、封禁等丰富插件  3、基于请求内容的分流\n 基于领域专有语言的分流规则，满足复杂业务场景定制化流量转发 支持完备的分流条件原语集，包括基于请求内容（URI/Header/Cookie等）以及请求上下文（IP、协议、标签、时间等）的条件原语。  4、灵活的负载均衡策略\n 支持集群级别负载均衡及实例级别负载均衡，实现多可用区容灾及过载保护 内置加权轮询、加权最小连接数策略，基于IP或请求内容识别用户实现会话保持  CNCF是云计算领域全球顶级的开源社区。BFE开源项目在2020年启动了加入CNCF的申请工作。经过一系列的准备工作，于2020年6月18日通过CNCF SIG-NETWORK的答辩，并在不到一周内收到了被CNCF TOC接受的通知。在加入CNCF后，BFE将改名为Beyond Front End。\nBFE开源技术已在百度内被HTTPDNS、云加速、BML等产品使用，并将和百度的云原生产品进一步深入结合。BFE商用产品已经被度小满、央视网等客户选用，并已经在多个客户进行了测试验证。BFE将进一步扩大开源范围，加强开源生态的建设，并基于开源建立百度负载均衡的商业生态。\n","id":6,"section":"posts","summary":"2020年6月24日，BFE开源项目被CNCF （Cloud Native Computing Foundation，云原生计算基金会）正式接纳为Sandbox Project","tags":["云原生"],"title":"百度BFE成为CNCF接纳网络方向上的第一个开源项目","uri":"https://www.devopsman.cn/2020/12/31/","year":"2020"},{"content":"今天给大家分享目前最popular的MySQL SQL审计平台Yearning，这个平台可以帮助开发者快速的完成SQL语句的语法的审核、检测、执行和回滚等操作。在早期，我们是先整理出来一套SQL使用规范，然后通过部门会议审核规范的内容，最后要求按照内容应用在实际的工作中，但是这些不免有些开发者依旧就不住或者不上心，不按套路出牌，造成数据库使用不统一。\nYearning自身包含了一套通常适用的审核规范，基本上能满足日常需要，同时规范了日常开发需求所涉及到的SQL变动，在Yearning平台的辅助下，日常的SQL变动也更加贴近SQL使用的规范化、标准化的要求，同时100%基于GO语言研发的Yearning也可以通过自定义二次开发(遵循AGPL协议)增加一些符合自己审核策略，但是它的审核引擎Juno不是开源的。\nYearning的前端是基于Vue.js构建的，而这块审核规则的代码也是全部在JS中传递和处理的，另外它还提供SQL语法高亮、自动补全和智能提示、可视化等。\n使用感受  有工单记录，让变更的SQL记录落库保存，易于审计。 开发者自己提交，监测最大程度减小以往通过微信传送等粘贴出错几率，给DBA.运维省心。 再一次证明落实规范性的东西需要有工具化、流程化，靠自觉万万是做不到的。 SQL审批要设置多成员多级审核，最后自己执行落库。  下面列出支持的主要功能:\nYearning Feature  SQL查询  查询工单 导出 自动补全，智能提示 查询语句审计   SQL审核  流程化工单 SQL语句检测与执行 SQL回滚 历史审核记录   推送  E-mail工单推送 钉钉webhook机器人工单推送   用户权限及管理  角色划分 基于用户的细粒度权限 注册   其他  todoList LDAP登录 动态审核规则配置   AutoTask自动执行  体验 Yearning的安装十分简单，它依赖一个mysql数据库用于存储工单的数据、回滚的SQL语句，所以需要先初始化数据库-m，然后在启动-s。对于回滚的语句不得不说一下，真的要是涉及到大变更的操作的时候，还是建议手动或者可靠地备份方式进行。官方也提供了安装手册，一般建议容器化部署，方便省事，安装包内也有Dockerfile，二次开发的同学也可以自己构建发布。\n在需要配置钉钉或者微信的时候，可以通过选项设置通知时显示Yearning的平台地址\n下面是配置钉钉和OpenLDAP登录的参考:\n","id":7,"section":"posts","summary":"今天给大家分享目前最popular的MySQL SQL审计平台Yearning，这个平台可以帮助开发者快速的完成SQL语句的语法的审核、检测、","tags":["SQL","安全","运维"],"title":"Yearning目前最流行的开源数据库审核平台","uri":"https://www.devopsman.cn/2020/12/30/","year":"2020"},{"content":"文章索引 在2020年即将过去的的最后几个小时，趁着大家都在看跨年晚会的时候，我整理了2020年云原生生态圈一整年写过的所有文章，一共147篇,方便于粉丝搜索与查看，作为一份小小的礼物回馈给哪些一直支持我的粉丝们吧。\n在特不平凡的2020年，从新冠疫情到现在，我们经历的太多，但不管如何2020年终将跨去，新的一年，新的心愿，新的希望，新的生活，我相信在新的一年一切都会变得更好，打工人，爷青回，奥利给！！！\n2021年，我们一起努力！嗨，打工人！！！祝愿大家新的一年，身体健健康康，工作顺利，万事称心如意！\nDocker  2020-03-15 Docker镜像分析工具之Dive介绍 2020-03-15 Docker镜像分析工具之Dive视频指南 2020-03-23 使用SSH协议远程操作Docker 2020-03-27 庆祝docker7周年 2020-06-15 Lazydocker:专为\u0026quot;懒人\u0026quot;设计命令行可视化工具 2020-08-16 Docker 禁止被列入美国“实体名单”的国家、企业、个人使用 2020-08-18 Docker Swarm上基于Consul实现高可用RabbitMQ集群 2020-09-13 深入了解容器技术是如何在大规模革新应用程序和微服务的打包和部署过程中发展起来的 2020-12-28 在Docker中使用nginx托管vue应用程序  Nginx  2020-07-10 Nginx专辑|01 - Nginx1.18.0 编译安装 2020-07-12 Nginx专辑|02 - Nginx1.18 静态服务器规范化配置 2020-07-12 Nginx专辑|03 - Nginx缓存代理模块之proxy_cache 2020-07-25 Nginx专辑|04 - 带命令行控制台的轻量级日志分析工具-GoAccess 2020-07-29 Nginx专辑|05 -如何使用Nginx配置正向代理 2020-12-29 在Nginx中支持HTTP3.0/QUIC 2020-10-11 Nginx专辑|08 - 基本安全优化之修改Nginx软件名和版本号  VPN  2020-06-20 OpenVPN高级进阶: 5个使用场景和问题集 2020-06-20 OpenVPN最难忘的一次”灾后重建“  Kubernetes  2020-03-25 Step By Step To Learn Kubernetes Series 2020-04-05 Kubernetes中Master和Node的概念 2020-04-05 昨天网站都变成灰色了，这其中是怎么实现的？ 2020-04-15 Traefik2.2:迎来黑暗模式与Udp LoadBalance 2020-02-25 当运维遇上了\u0026hellip; 2020-05-07 在Kubernetes上使用Openldap做集中认证 2020-05-11 利用vscode插件icepanel可视化配置运行在k8s上的微服务 2020-07-01 百度开源BFE被CNCF接纳为Sandbox Project 2020-08-20 Calico: 助力Kubernetes的纯三层网络解决方案 2020-08-27 Kubernetes v1.19来了！亮点抢先看 2020-09-03 云原生键值数据库TiKV毕业了 2020-11-04 2020年Kubernetes中7个最佳日志管理工具 2020 15 个 Kubectl 现有命令使用技巧 - 拿来即用 2020-08-22 2020年最高效的10款Kubernetes助力神器 2020-06-29 最流行的五款Kubernetes交互式可视化工具 2020-11-02 K8s节点故障：资源控制器会触发哪些动作 2020-11-05 几张图就把 Kubernetes Service 掰扯清楚了 2020-11-08 6张图循序渐进讲透Kubernetes Ingress资源对象 2020-11-15 白话文图解Pod在Kubernetes中的创建过程 2020-06-08 Jenkins在kubernetes上的初体验 2020-08-29 实践 | Kubernetes守护进程集之DaemonSet 2020-07-26 神奇！如何快速成为一名优秀的YAML工程师？ 2020-05-30 ab压力测试模拟实现kubernetes Pod水平自动伸缩 2020-10-18 Kubernetes五个实用的自动化运维脚本 -香~ 2020-10-20 15 个 Kubectl 现有命令使用技巧 - 拿来即用 2020-12-03 @所有人: 不要慌，Kubernetes和Docker没分手 2020-12-11 2020年最后一个Kubernetes版本 v1.20 重磅发布 2020-12-16 听说会Traefik就一定会Ingress-nginx 2020 用Prometheus对业务服务进行监控 2020 Prometheus监控系列-监控篇 2020-06-12 kubernetes炼气期之掌握Kubernetes的背景 2020 Prometheus监控系列-部署篇 2020 写给孩子看的Kubernetes动画指南  2020 kubernetes的ingress控制器比较(traefik2.0.5安装指南) 2020-03-07 kubernetes深度探究Node和Pod的亲和性和反亲和性 2020 在kuebernetes上通过nfs-server持久化postgresql 2020-03-26 kubernetes监控架构核心组件Metrics-server 2020 基于BasicAuth认证的Traefik2.0.5 2020 在Kubernetes上部署Nginx Ingress controller 2020 kubernetes安装方案大全 2020 kubernetes最常用的资源对象Deployment 2020 kubernetes炼气期之掌握kuebernetes背景 2020-06-12 kubernetes炼气期之k8s平台快速搭建 2020 二进制部署Kubernetes集群1.9版本  DevOps技术栈  2020-03-05 DevOps全开源端到端部署流水线视频 2020-03-28 2020 DevOps工程师面试 33 问 2020带命令行控制台的轻量级日志分析工具-GoAccess 2020-07-20 Hugo\u0026amp;Zzo-部署静态网站设计的一些思考 2020-07-16 10万+人在用的组网解决方案Wireguard：实践篇 2020-07-10 运维知识体系v3.1 \u0026amp; Web缓存知识体系v3.0 2020-07-09 Jenkinsh专辑|解决Jenkins安装的疑难杂症 2020-07-07 Harbor开源镜像仓库企业级实践(1/3) 2020-07-12 Harbor2.0开源镜像仓库企业级实现 2020-07-09 Harbor2.0配置高可用的Harbor镜像仓库 2020-07-05 阿里云出品·Kubernetes 深入浅出实践 v1.0 2020-05-18 微软出品·Kubernetes 最新学习指南 v3.0 2020-07-04 火焰图：全局视野的 Linux 性能剖析 1k+在读 2020最流行的五款 Kubernetes 交互式可视化工具 900+在读 2020-06-26 轻松爬取拉勾网岗位招聘信息 600+在读 2020-06-25 Yearning - 最 Popular 的 MYSQL 审计平台 700+在读 2020-10-21 2020年 DevOps或SRE循序渐进学习路线图 🤝收藏版(上) 2020-10-21 2020年 DevOps或SRE循序渐进学习路线图 🤝收藏版(上) 2020-11-26 2020年末分享 DevOps或SRE循序渐进学习路线图 🤝 收藏版 2020-12-19 突破限制！！！尝试Github Actions 2020-03-21 Jenkins流水线动画 2020-06-08 自定义Build History中显示构建信息 2020-03-01 自定义构建Jenkins镜像实战 2020-06-08 Jenkins在kubernetes上的初体验 2020-04-22 Jenkins动态Slave在k8s上的实践 2020-06-05 GitHub+Travis+Mkdocs构建文档库  Ansible  2020-02-16 Ansible书籍分享(ansible for devops/ansible for kubernetes) 2020-11-01 没有集群照样学Ansible:托管的容器环境(实用)  GitLab  2020-02-16 Gitlab Runner系列-环境部署篇 2020-02-16 Gitlab Runner系列-持续集成篇 2020-02-16 Gitlab Runner系列-持续部署篇 2020-06-06 用GitLAB CICD pipeline template持续集成 2020-10-24 从GitLabCE CI/CD方法论中探索实践 2020-10-25 Gitlab CICD 与Kubernetes实践·部署GitLab 2020-10-26 GitlabCI与Kubernetes实践·部署GitLab-Runner 2020-10-27 Gitlab持续集成中Dood与Dind应该怎么玩？ 2020-10-27 GitLab CICD与Kubernetes实践·部署Flask Web服务 2020-12-20 使用Gitlab Template增强GitLab CICD的扩展性和兼容性   Prometheus  2020-03-20 Prometheus监控系列-部署篇 2020-03-20 Prometheus监控系列-监控篇(consul) 2020-03-20 Awesome Alertmanager Rule 2020-04-06 OpenTracing入门与 Jaeger的实现 2020-09-22 Cortex: 高可用和水平扩展Prometheus监控系统  Go  2020-05-12 ​GoLANG IN ACTION 2020 2020-09-22 Goland中使用Golang命令行工具 2020-05-14 使用Go案例完成开发到部署实践 2020-10-06 Go - flag：命令行flags解析  Python  2020-02-21 Python原来如此美丽之Requeat \u0026amp; Parsel 2020-02-24 域名有效期监控的最佳方案 2020-07-11 Python自动化运维最佳实践的两本书  linux  2020-08-23 不限速、大容量、定制化个人网盘 2020-02-28 企业级规范部署中央认证软件Openldap 2020-02-24 域名有效期监控 2020-03-14 Awk权威指南之终结篇 2020-03-11 Awk权威指南 2020-06-13 打造Mac下高颜值好用的终端环境 2020-10-06 走进Network Namespace学会容器网络调试 2020-06-01 提高工作效率，推荐 10款命令行工具 2020-07-21 DON\u0026rsquo;T SIGKILL 2020-07-22 漫画: life in a web server 2020-08-05 给GitHub \u0026ldquo;彩蛋\u0026rdquo; readme 生成自定义统计信息 2020-08-23 企业多人协同办公软件-Confluence 7.6体验 2020-10-13 网络工具大全·高清 2020-11-25 tcpdump: 我来帮你过滤和分析系统中的网络数据 2020-11-27 坑已踩完 - JIRA v8.7服务迁移实践方案 2020-12-11 Red Hat 杀死了CentOS后 Rocky Linux 面世 2020-12-19 hostctl · 像PRO一样管理你的hosts文件 2020-11-29 网络篇：朋友面试之TCP/IP，回去等通知吧 2020-11-11 一文讲透LVM原理到实践 2020-08-18 面试题：负载均衡方案Lvs与Nginx的优缺点 2020-07-30 22 个好用的 CLI 工具 2020-06-09 wordpress - 运维必知的博客系统平台  Redis  2020-04-16 到了弃用Redis-sentinel架构的时候了  Consul  2020-04-13 Consul1.7 多数据中心 新Hashicorp学习指南 2020-05-29 突发！Terraform、Consul、Vagrant 等禁止中国使用！  Zookeeper  2020-04-20 一览zookeeper3.6.0新特性  MySQL  2020-09-17 MYSQL数据同步之基于GTID事务数据同步 2020-09-18 MySQL · 物理备份 · XtraBackup备份原理 2020-12-01 MYSQL MGR单主模式集群容器化实践  tools  2020-03-13 IDE你要的永远激活在这里 2020-05-08 YAPI-高效、易用、功能强大的 api 管理平台 2020-06-07 Logoly-P站\u0026amp;YouTobe风格Logo在线生成器  微信群  2020-04-16 DevOps\u0026amp;CloudNative技术交流群  ","id":8,"section":"posts","summary":"文章索引 在2020年即将过去的的最后几个小时，趁着大家都在看跨年晚会的时候，我整理了2020年云原生生态圈一整年写过的所有文章，一共147篇","tags":["年度文章"],"title":"云原生生态圈·2020年微信公众号全年文章目录索引","uri":"https://www.devopsman.cn/2020/12/29/","year":"2020"},{"content":"Kubernetes是容器编排的实际标准。Kubernetes作为首选的容器解决方案，已经迅速流行起来。我们来看看Kubernetes的10个最好的工具。这些应用程序将补充K8s并增强您的开发工作，以便您可以从您的Kubernetes获得更多。利用这些Kubernetes的小\u0026quot;伙伴\u0026quot;来简化应用程序定义、迁移集群、简化云部署等等\nKubernetes Dashboard Kubernetes Dashboard是一种基于web的Kubernetes监视工具，它更适合于较小的集群。它提供了一个UI来管理k8s。这些任务包括发现、负载平衡和监控。有许多选项可用于故障排除。仪表板允许监视聚集的CPU和内存使用情况。它可以监控工作负载的运行状况。安装很简单，因为有现成的YAML模板可用。Cabin是移动版的kubernetes dashboard，它也为Android和iOS提供了类似的功能。\nKubelet 这些是运行在Kubernetes集群的每个节点上的Node Agents。他们可以向中心API服务器注册节点。一个PodSpec. 它是一个用来描述每个pod的YAML或JSON对象。因此，Kubelet能够通过其PodSpecs监视这些节点。\nHelm Kubernetes Helm是一个管理预配置Kubernetes资源包的工具，又名Kubernetes图表。使用Helm可以用来：\n 查找并使用打包成Kubernetes chart的流行软件 以Kubernetes图表的形式共享您自己的应用程序 创建Kubernetes应用程序的可重复构建部署 智能地管理您的Kubernetes清单文件 管理Helm包的发布  Gravity 如果您希望将应用程序部署到Kubernetes中，许多应用程序都有helm charts来指导和自动化该过程。但是，如果您希望按原样使用Kubernetes集群并将其部署到其他地方，该怎么办呢?\nGravity为Kubernetes集群、它们的容器仓库、它们正在运行的应用程序(称为捆绑的应用程序)拍摄快照。这个包(只是一个.tar文件)可以在Kubernetes运行的任何地方复制集群。\nGravity还确保目标基础设施能够支持与源集群相同的行为需求，并且目标上的Kubernetes运行时符合要求。\nMinikube Minikube允许您在本地安装和试用Kubernetes。该工具是Kubernetes探索的一个很好的起点。在您的笔记本上的虚拟机(VM)中轻松地启动单节点Kubernetes集群。Minikube可以在Windows、Linux和OSX上使用。在短短5分钟内，您将能够探索Kubernetes的主要功能。只需一个命令就可以直接启动Minikube仪表板。\nKubespray Kubespray为Kubernetes的部署和配置提供了一组可操作的Ansible角色。Kubespray可以使用AWS、GCE、Azure、OpenStack或裸金属基础设施作为服务(IaaS)平台。Kubespray是一个具有开放开发模型的开源项目。对于已经了解Ansible的用户来说，这个工具是一个很好的选择，因为不需要使用其他工具来进行配置和编排。\nTwistlock Twistlock是一个全生命周期的容器安全解决方案。它有一个VMS，可以扫描任何易受攻击的区域。它可以在一定的基础上持续的扫描Kubernetes,还有一个自动防火墙。扫描容器图像是Twistlock的另一个重要特性。支持Node.js组件和Docker图像。Twistlock专注于容器安全性的两个基本方面，首先，它会持续扫描容器图像，原因是每天都有新的威胁数据出现;另一个重点是容器运行的安全性。首先必须设置正常行为的基线。之后，就可以很容易地进行监测。\nKubectl Kubectl是Kubernetes的默认CLI工具。它支持与Kubernetes相关的所有操作。通过$HOME目录中的配置文件发现节点。kubectl也接受其他kubeconfig文件。只需要设置相关的环境变量。这也可以用kubeconfig flag来完成。Docker用户可以使用kubectl与API服务器交互。kubectl命令与Docker命令类似。只有一些小的差别。\nAlcide kAudit Alicide kAudit，通过扫描Kubernetes的审计日志，通过扫描Kubernetes审核日志来实时自动分析多集群部署中的违规，滥用和异常行为。该解决方案向客户提供检测到的异常的摘要以及访问，使用和性能趋势。\nKubeless Kubeless该工具是本地Kubernetes工具，用于部署小型应用程序。它使用Kubernetes资源来启用许多任务。这些有助于自动扩展，路由API，进行监视和故障排除。Kubeless脱颖而出，因为它支持自定义资源定义。该功能允许Kubeless创建自定义的Kubernetes资源。然后，您可以使用集群内控制器对其进行监视。它允许您根据需要启动运行时。然后，这些运行时可以通过HTTP使用。此外，还可以使用PubSub机制。\n","id":9,"section":"posts","summary":"Kubernetes是容器编排的实际标准。Kubernetes作为首选的容器解决方案，已经迅速流行起来。我们来看看Kubernetes的10","tags":["容器","消息队列"],"title":"2020年10个最佳的Kubernetes工具","uri":"https://www.devopsman.cn/2020/08/18/","year":"2020"},{"content":"实验中通过Docker swarm建立RabbitMQ集群，同时我们将专注于基于Consul的RabbitMQ集群。我们还将添加HAProxy服务器场以负载均衡AMQP请求，并提高群集的整体可用性。以下是最终的逻辑拓扑。\nSo, Let’s get started! 👊\n安装docker swarm集群    主机名 IP     master 192.168.99.128   Node1 192.168.99.133   Node2 192.168.99.134    安装docker curl -fsSL get.docker.com -o get-docker.sh CHANNEL=stable sh get-docker.sh --mirror Aliyun root@master:~# docker -v Docker version 19.03.12, build 48a66213fe 初始化docker swarm集群  在master上初始化集群  #Init Docker Swarm - run on master root@master:~# docker swarm init --advertise-addr 192.168.99.128 Swarm initialized: current node (htu24tq7mhip4ca7a9uqnndef) is now a manager. To add a worker to this swarm, run the following command: docker swarm join --token SWMTKN-1-1ifiwgcc2k4ztyr80zm6e3vj7ewbbi1fqfnba1vfisesnos6o4-es73sy0dx4ie5lbivx90mcbky 192.168.99.128:2377 To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.  node1加入集群  root@node1:~# docker swarm join --token SWMTKN-1-1ifiwgcc2k4ztyr80zm6e3vj7ewbbi1fqfnba1vfisesnos6o4-es73sy0dx4ie5lbivx90mcbky 192.168.99.128:2377 This node joined a swarm as a worker. root@master:~# docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION htu24tq7mhip4ca7a9uqnndef * master.devopsman.cn Ready Active Leader 19.03.12 vlfb0ibx4k1z41r76yhgpummk node1.devopsman.cn Ready Active 19.03.12 root@master:~# docker node promote node1.devopsman.cn Node node1.devopsman.cn promoted to a manager in the swarm.  node2加入集群  root@node2:~# docker swarm join --token SWMTKN-1-1ifiwgcc2k4ztyr80zm6e3vj7ewbbi1fqfnba1vfisesnos6o4-es73sy0dx4ie5lbivx90mcbky 192.168.99.128:2377 This node joined a swarm as a worker. root@master:~# docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION htu24tq7mhip4ca7a9uqnndef * master.devopsman.cn Ready Active Leader 19.03.12 vlfb0ibx4k1z41r76yhgpummk node1.devopsman.cn Ready Active Reachable 19.03.12 99f5xkfepct301n48tqqslz22 node2.devopsman.cn Ready Active 19.03.12 root@master:~# docker node promote node2.devopsman.cn Node node2.devopsman.cn promoted to a manager in the swarm. root@master:~# docker network create --driver=overlay --attachable prod q5rzw0i1xdh9kovytjeg0sj9p root@master:~# docker network ls NETWORK ID NAME DRIVER SCOPE ba6f05d438cf bridge bridge local ee47cd6a4247 docker_gwbridge bridge local 8df9d4ecb5be host host local rl5o9jz6fxp8 ingress overlay swarm 25lk998untvk local overlay swarm 3a66aac75dbf none null local q5rzw0i1xdh9 prod overlay swarm 部署consul集群 注意点\n 持久化consul的数据，避免数据丢失 在global模式下部署Consul服务，并通过节点标签管理服务调度 使用两个单独的网络，一个用于内部consul的通信，另一个用于RabbitMQ与Consul服务之间的通信  consul部署清单 version: '3.6' services: consul: image: consul:1.4.0 hostname: \u0026quot;{{.Node.Hostname}}\u0026quot; networks: - consul - prod ports: - 8400:8400 - 8500:8500 - 8600:53 volumes: - consul-data:/consul/data deploy: mode: global placement: constraints: [node.labels.consul == true] command: [ \u0026quot;agent\u0026quot;, \u0026quot;-server\u0026quot;, \u0026quot;-bootstrap-expect=3\u0026quot;, \u0026quot;-retry-max=3\u0026quot;, \u0026quot;-retry-interval=10s\u0026quot;, \u0026quot;-datacenter=prod\u0026quot;, \u0026quot;-join=consul\u0026quot;, \u0026quot;-retry-join=consul\u0026quot;, \u0026quot;-bind={{ GetInterfaceIP \\\u0026quot;eth0\\\u0026quot; }}\u0026quot;, \u0026quot;-client=0.0.0.0\u0026quot;, \u0026quot;-ui\u0026quot;] networks: consul: prod: external: true volumes: consul-data: 初始化consul集群 docker node update --label-add consul=true node1.devopsman.cn node1.devopsman.cn docker stack deploy -c docker-compose_consul.yaml consul # 等待10s root@master:~/consul_rabbitmq_docker# docker node update --label-add consul=true master.devopsman.cn master.devopsman.cn root@master:~/consul_rabbitmq_docker# docker node update --label-add consul=true node1.devopsman.cn node1.devopsman.cn root@master:~/consul_rabbitmq_docker# curl 192.168.99.134:8500/v1/status/leader \u0026quot;10.0.3.5:8300\u0026quot; root@master:~/consul_rabbitmq_docker# curl 192.168.99.134:8500/v1/status/peers [\u0026quot;10.0.3.5:8300\u0026quot;,\u0026quot;10.0.3.7:8300\u0026quot;,\u0026quot;10.0.3.9:8300\u0026quot;] root@master:~/consul_rabbitmq_docker# 在浏览器打开http://192.168.99.128:8500访问consul的dashboard来验证安装是否成功。\n部署RabbitMQ集群 注意点\n 持久化数据防止数据丢失。 在global模式下部署RabbitMQ服务，并通过节点标签管理服务调度 使用Prod网络进行内部/外部RabbitMQ通信 不要暴露 RABBITMQ_ERLANG_COOKIE and RABBITMQ_DEFAULT_PASS 主机名很重要，因为RabbitMQ使用主机名作为数据目录  RabbitMQ部署清单 version: \u0026quot;3.6\u0026quot; services: rabbitmq-01: image: olgac/rabbitmq:3.7.8-management hostname: rabbitmq-01 environment: - RABBITMQ_DEFAULT_USER=admin - RABBITMQ_DEFAULT_PASS=Passw0rd - RABBITMQ_ERLANG_COOKIE=\u0026quot;MY-SECRET-KEY-123\u0026quot; networks: - prod volumes: - rabbitmq-01-data:/var/lib/rabbitmq deploy: mode: global placement: constraints: [node.labels.rabbitmq1 == true]rabbitmq-02: image: olgac/rabbitmq:3.7.8-management hostname: rabbitmq-02 environment: - RABBITMQ_DEFAULT_USER=admin - RABBITMQ_DEFAULT_PASS=Passw0rd - RABBITMQ_ERLANG_COOKIE=\u0026quot;MY-SECRET-KEY-123\u0026quot; networks: - prod volumes: - rabbitmq-02-data:/var/lib/rabbitmq deploy: mode: global placement: constraints: [node.labels.rabbitmq2 == true]rabbitmq-03: image: olgac/rabbitmq:3.7.8-management hostname: rabbitmq-03 environment: - RABBITMQ_DEFAULT_USER=admin - RABBITMQ_DEFAULT_PASS=Passw0rd - RABBITMQ_ERLANG_COOKIE=\u0026quot;MY-SECRET-KEY-123\u0026quot; networks: - prod volumes: - rabbitmq-03-data:/var/lib/rabbitmq deploy: mode: global placement: constraints: [node.labels.rabbitmq3 == true]networks: prod: external: true volumes: rabbitmq-01-data: rabbitmq-02-data: rabbitmq-03-data: config/enabled_plugins [rabbitmq_management, rabbitmq_peer_discovery_consul, rabbitmq_federation, rabbitmq_federation_management, rabbitmq_shovel, rabbitmq_shovel_management]. config/rabbitmq.conf loopback_users.admin = false cluster_formation.peer_discovery_backend = rabbit_peer_discovery_consul cluster_formation.consul.host = consul cluster_formation.node_cleanup.only_log_warning = true cluster_formation.consul.svc_addr_auto = true cluster_partition_handling = autoheal#Flow Control is triggered if memory usage above %80. vm_memory_high_watermark.relative = 0.8#Flow Control is triggered if free disk size below 5GB. disk_free_limit.absolute = 5GB RabbitMQ集群初始化 root@master:~/consul_rabbitmq_docker# docker node update --label-add rabbitmq1=true master.devopsman.cn master.devopsman.cn root@master:~/consul_rabbitmq_docker# docker node update --label-add rabbitmq2=true node1.devopsman.cn node1.devopsman.cn root@master:~/consul_rabbitmq_docker# docker node update --label-add rabbitmq3=true node2.devopsman.cn node2.devopsman.cn root@master:~/consul_rabbitmq_docker# docker stack deploy -c docker-compose_rabbitmq.yml rabbitmq Creating service rabbitmq_rabbitmq-03 Creating service rabbitmq_rabbitmq-01 Creating service rabbitmq_rabbitmq-02 root@master:~/consul_rabbitmq_docker# 配置haproxy代理端口 haproxy部署清单 version: \u0026quot;3.6\u0026quot; services: haproxy: image: olgac/haproxy-for-rabbitmq:1.8.14-alpine ports: - 15672:15672 - 5672:5672 - 1936:1936 networks: - prod deploy: mode: global networks: prod: external: true config/haproxy.cfg global log 127.0.0.1 local0 log 127.0.0.1 local1 notice maxconn 4096 defaults log global option tcplog option dontlognull timeout connect 6s timeout client 60s timeout server 60s listen stats bind *:1936 mode http stats enable stats hide-version stats realm Haproxy\\ Statistics stats uri /listen rabbitmq bind *:5672 mode tcp server rabbitmq-01 rabbitmq-01:5672 check server rabbitmq-02 rabbitmq-02:5672 check server rabbitmq-03 rabbitmq-03:5672 checklisten rabbitmq-ui bind *:15672 mode http server rabbitmq-01 rabbitmq-01:15672 check server rabbitmq-02 rabbitmq-02:15672 check server rabbitmq-03 rabbitmq-03:15672 check 验证RabbitMQ集群 访问 http://192.168.99.128:15672 RabbitMQ的控制后台，密码为:(admin/Passw0rd)\n最后，查看一下haproxy的后台。\n Check http://192.168.99.128:1936  ","id":10,"section":"posts","summary":"实验中通过Docker swarm建立RabbitMQ集群，同时我们将专注于基于Consul的RabbitMQ集群。我们还将添加HAProxy","tags":["容器","消息队列","运维"],"title":"Docker swarm实现基于Consul和Haproxy的RabbitMQ高可用集群","uri":"https://www.devopsman.cn/2020/08/18/","year":"2020"},{"content":"介绍几种开源的网盘存储解决方案，可以在开源的网盘服务基础上，结合一些某云的存储服务，实现私人定制化个人网盘。\nNextcloud [NextCloud](\u0026ldquo;https://docs.nextcloud.com/\u0026quot; Nextcloud)是一个完全开源的个人网盘，部署简单，可以直接通过Docker进行部署和使用，同时支持手机和电脑端的客户端远程访问上传和下载:\n安装部署 mkdir nextcloud docker run -d --name nextcloud -p 8080:80 -v ~/nextcloud:/var/www/html nextcloud 部署完成之后，直接访问\u0026quot;http://localhost:8080\u0026quot;即可\n完成注册之后，即可进去使用，下面是安装成功之后，上传文件的效果图:\nOwncloud ownCloud]是一个开源免费的存储管理工具，它能帮你快速架设一套专属的网盘服务，可以像 Dropbox 那样实现文件跨平台同步、共享、版本控制、团队协作等等\nOwncloud功能  存储：图片，文档，视频，通讯录以及其他等等 客户端支持：Android，IOS,MaxOS,Windows,Web,Linux 分享：可以直接共享直接链接给朋友 在线看视频，文档，音乐。 非常适合作为家庭网络存储中心，全家共享 可以自行修改功能（作为开发者）  如果在阿里云上，可以结合阿里云的oss文件存储搭建个人私有网盘，配置文件owncloud.yml\nowncloud: image: owncloud:9 restart: always links: - mysql:mysql volumes: - owncloud:/owncloud labels: aliyun.routing.port_80: \u0026#39;owncloud\u0026#39; mysql: image: mysql:5.7 restart: always environment: MYSQL_ROOT_PASSWORD: changeme 开始部署owncloud\ndocker-compose -f owncloud.yml up -d 安装初始化完成之后的界面:\nseafile   Seafile 支持端到端的加密技术来保护你的数据安全\n  Seafile 支持基于角色的用户管理，Seafile 支持LDAP/AD集成。用户可以通过邮箱或者 Windows 用户名来登录。AD 中的群组也可以同步到 Seafile 中。\n  Seafile 支持定期的数据备份 (通过 mysqldump 和 rsync). Seafile 也支持运行一台备份服务器，将主服务器上的数据实时备份到备份服务器上。\nSeafile 包含 seaf-fsck 工具来帮助管理员来检查和移除损坏的数据。fsck 工具也支持在没有数据库的情况下导出原始文件。\n  Seafile 包含 WebDAV 接口。你可以通过该接口和其他移动应用集成。\nSeafile 也包含 HTTP 的 REST API 来和第三方应用整合。\n  大家可以自行研究，按需选择。\n","id":11,"section":"posts","summary":"介绍几种开源的网盘存储解决方案，可以在开源的网盘服务基础上，结合一些某云的存储服务，实现私人定制化个人网盘。 Nextcloud [NextCloud](\u0026ldquo;https://docs.nextcloud.com/\u0026quot; Nextcloud)是一个完","tags":["运维"],"title":"如何搭建适合自己的个人网盘","uri":"https://www.devopsman.cn/2020/08/18/","year":"2020"},{"content":"静态网站的个人设计的个人思考 为什么要搭建自己的静态博客系统？  写好的Markdown可以快速的发布到自己的博客网站上，不需要什么一键式多平台发布、自媒体自动同步(支持的不全面) 没有广告，页面干净酸爽 高度自由化，支持定制化css样式，可以了解和实践一些前端的知识。 事件驱动学习，养成持续学习和分享的习惯，收纳总结常遇到的问题以及处理经验，毕竟有些东西慢慢的会忘掉的。 通过公开的自己的笔记，暴露问题，从而发现问题，培养和理清自己思考问题的思路  博客网站的构建与部署思考 先看一下初期的设计图如下，将生成好的静态页面推送到代码仓库里面，然后通过GITHUB自带的GitHub Action自动的部署到云端主机上，同时将域名CNAME或者A到Github Pages是上或者云主机IP上。 定制化个人博客 我们这里使用Hugo这个静态网站生成工具来制作静态网站，该工具是通过Go编写的，因此小巧且资源占用少。下面本次实验的基础环境 软件|版本 \u0026ndash;|\u0026ndash; 系统|Macos 10.15.5 Hugo|hugo v0.73.0 Zzo theme|Zzo 在下载完成hugo之后，将其添加到系统的可执行的路径下,echo $PATH获取。\nhugo new site linuxermaster.github.io # 创建一个名称为linuxermaster.github.io的博客工作目录 cd linuxermaster.github.io git submodule add https://github.com/zzossig/hugo-theme-zzo.git themes/zzo git submodule update --remote --merge hugo server --themesDir ../.. # 即可运行一个使用zzo主题的静态网站 以上就完成了静态网站的第一小步，下面继续了解hugo\n├── archetypes │ └── default.md ├── config │ └── _default # 存储默认配置文件的目录，里面包含全局配置、菜单配置、语言配置、其他诸如友链等通信配置 ├── content │ ├── en # 存储英文版博客的目录,这里就是存储markdown文件的目录 │ └── zh # 存储中版博客的目录,这里就是存储markdown文件的目录 ├── data │ ├── font.toml # 设置字体 │ └── skin.toml # 设置皮肤的颜色 ├── deploy.sh # 推送静态网页到git的脚本 ├── layouts │ └── partials ├── public # 执行hugo命令将markdown转换成html的静态页面目录 │ ├── CNAME │ ├── css │ ├── en │ ├── favicon │ ├── favicon.ico │ ├── favicon.png │ ├── fonts │ ├── gallery │ ├── images │ ├── index.html │ ├── js │ ├── lib │ ├── logo-192.png │ ├── logo-512.png │ ├── logo.png │ ├── manifest.json │ ├── sitemap.xml │ └── zh ├── resources │ └── _gen ├── static # 静态网站需要的一些样式文件目录 │ ├── en │ ├── favicon │ ├── fonts │ ├── gallery │ └── images └── themes # 网站需要的模板主题目录 └── zzo 可以将以前写的markdown存放在content/en/目录下,然后运行网站就可以看到你的笔记\n☸️ k8sdev🔥 default ~/workspaces/linuxermaster.github.io   master ●✚  🐳 👉 hugo server -t zzo | EN | ZH -------------------+-----+------ Pages | 248 | 52 Paginator pages | 10 | 0 Non-page files | 281 | 3 Static files | 136 | 136 Processed images | 0 | 0 Aliases | 56 | 8 Sitemaps | 2 | 1 Cleaned | 0 | 0 Built in 6102 ms Watching for changes in /Users/marionxue/workspaces/linuxermaster.github.io/{archetypes,content,data,layouts,static,themes} Watching for config changes in /Users/marionxue/workspaces/linuxermaster.github.io/config/_default Environment: \u0026#34;development\u0026#34; Serving pages from memory Running in Fast Render Mode. For full rebuilds on change: hugo server --disableFastRender Web Server is available at http://localhost:1313/ (bind address 127.0.0.1) Press Ctrl+C to stop 如何定制文章的简介和图片 很简单，只需要在markdown的首部添加以下yaml格式的配置说明即可:\nauthor: \u0026#34;云原生生态圈\u0026#34; # 文章标题下显示的作者名字 title: \u0026#34;Hugo+Zzo主题快速搭建持久维护的个人网站\u0026#34; # 文章的标题 date: 2020-07-20T19:49:06+09:00 # 自定义文章的发表时间 description: \u0026#34;私人订制你的技术平台平台\u0026#34; # 文章的说明描述 authorImage: \u0026#34;/images/whoami/头像.png\u0026#34; # 作者的头像 authorDesc: \u0026#34;啥都不会的DevOps工程师😂😂\u0026#34; # 作者的自述 draft: false # 是否标记为草稿，如果为true,将不会编译进html页面 hideToc: false # 是否隐藏toc目录 enableToc: true # 是否启用toc enableTocContent: false # 是否显示toc authorEmoji: 🔥 # 显示在作者名字之前的表情 tags: # 文章标签,用于搜索 - hugo - blog - zzo - hugo template image: images/posts/hugo-blog.png # 显示文章的图片 socialOptions: # 显示的一些社交账号信息 email: \u0026#34;marionxue@qq.com\u0026#34; facebook: \u0026#34;@marionxue\u0026#34; 效果如下如:\n如何自定义网站底部样式 自定义网站底部的样式，需要修改css样式,由于篇幅的问题，这篇文章就不在说明了。\n推送静态网站脚本 #!/bin/sh  # If a command fails then the deploy stops set -e printf \u0026#34;\\033[0;32mDeploying updates to GitHub...\\033[0m\\n\u0026#34; # Build the project. hugo # if using a theme, replace with `hugo -t \u0026lt;YOURTHEME\u0026gt;` # Go To Public folder cd public # Add changes to git. git add . # Commit changes. msg=\u0026#34;rebuilding site $(date)\u0026#34; if [ -n \u0026#34;$*\u0026#34; ]; then msg=\u0026#34;$*\u0026#34; fi git commit -m \u0026#34;$msg\u0026#34; # Push source and build repos. git push origin master% 正如前面的网站设计图所示，不管将网站托管到Github Pages上还是部署到云主机上,我们都会遇到访问慢的情况，因此我们可以通过域名加速缓解这个问题,比如通过又拍云推广合作等，写技术类的文章，避免不了需要一些图来辅助解释说明，因此可以将图片放在一些对象存储上，例如七牛，oss等，我这里暂时就存在七牛上，但是七牛的bucket绑定个人域名需要备案，所以首先需要有一个备案的域名，域名备案需要一台云主机，我使用的是滴滴云([1c/2G]/68元/年),滴滴云的特惠链接:https://i.didiyun.com/29WG0vRdqDG\n","id":12,"section":"posts","summary":"静态网站的个人设计的个人思考 为什么要搭建自己的静态博客系统？ 写好的Markdown可以快速的发布到自己的博客网站上，不需要什么一键式多平台发","tags":["博客"],"title":"Hugo-部署静态网站设计的一些思考","uri":"https://www.devopsman.cn/2020/07/20/","year":"2020"},{"content":"前段时间，发现新运维社区的赵班长(赵舜东)更新了一版《运维知识体系 v3.1》其中新增了一些目前比较火的、成熟的运维解决方案，也包括容器编排，微服务框架lstio等。分层的归类总结各个层次常用的开源软件以及服务组合解决方案，其中包括客户端层、外部层、网络层、接入层、应用服务层、存储层、基础服务层、容器层、操作系统层、基础设施层等，以及技术在运维、运维自动化成长方向的发展以及迭代的趋势，这里贴出运维知识体系-v3.1以及Web缓存知识体系-V3.0供大家学习参考。\n运维知识体系-v3.1 Web缓存知识体系-V3.0  图片素材来源: https://www.unixhot.com/\n 精彩文章回顾  Jenkinsh专辑|解决Jenkins安装的疑难杂症 Harbor2.0开源镜像仓库企业级实现 Harbor2.0配置高可用的Harbor镜像仓库 阿里云出品·Kubernetes 深入浅出实践 v1.0 微软出品·Kubernetes 最新学习指南 v3.0 火焰图：全局视野的 Linux 性能剖析 1k+在读 最流行的五款 Kubernetes 交互式可视化工具 900+在读 轻松爬取拉勾网岗位招聘信息 600+在读 Yearning - 最 Popular 的 MYSQL 审计平台 700+在读  ","id":13,"section":"posts","summary":"前段时间，发现新运维社区的赵班长(赵舜东)更新了一版《运维知识体系 v3.1》其中新增了一些目前比较火的、成熟的运维解决方案，也包括容器编排，","tags":["Kubernetes","网络","运维"],"title":"运维知识体系-v3.1","uri":"https://www.devopsman.cn/2020/07/10/","year":"2020"},{"content":"文章背景 日常的工作中，会收到一堆CPU使用率过高的告警邮件，遇到某台服务的CPU被占满了，这时候我们就要去查看是什么进程将服务器的CPU资源占用满了。通常我们会通过top或者htop来快速的查看占据CPU最高的那个进程，如下图：\n这里是通过一个普通的服务器做演示使用，如图所示当前服务器占用CPU最高的是一个叫做kube-apiserver命令运行的一个进程，该进程的PID为25633,当然你可能遇到一个服务器上运行有多个服务，想快速知道占用率最高的那几个进程的话，你可以使用以下命令:\nps aux|head -1;ps -aux | sort -k3nr | head -n 10 //查看前10个最占用CPU的进程 ps aux|head -1;ps -aux | sort -k4nr | head -n 10 //查看前10个最占用内存的进程 但是通过以上的方法获取到服务器占用资源的进程之后，还是不知道CPU使用究竟耗时在哪里,不清楚瓶颈在哪里，此时就可以通过Linux系统的性能分析工具perf分析，分析其返回的正在消耗CPU的函数以及调用栈。然后可以通过解析perf采集的数据，渲染到火焰图🔥，就清楚的知道究竟占用系统CPU资源的罪魁祸首了。\n在制作火焰图之前，需要先来说说这个Linux性能分析工具perf,该工具是一个相对简单易上手的性能分析工具，是Performance单词的缩写，通过其perf的命令选项完成系统事件的采集到解析，我们来简单的认识一下：\nlinux上的性能分析工具Perf 安装perf 我目前的服务器发行版是Ubuntu 16.04.6 LTS因此需要先安装perf才能使用，该工具由linux-tools-common提供，但是它需要安装后面的依赖。\n#安装 root@master:~# apt install linux-tools-common linux-tools-4.4.0-142-generic linux-cloud-tools-4.4.0-142-generic -y root@master:~# perf -v #显示perf的版本 perf version 4.4.167 在安装完成时候，我们就可以对上图CPU使用率最高的进程ID为25633的进程进行采样分析。\n首选我们采集一下该进程的调用栈信息:\nroot@master:~# sudo perf record -F 99 -p 25633 -g -- sleep 30 [ perf record: Woken up 1 times to write data ] [ perf record: Captured and wrote 0.039 MB perf.data (120 samples) ] 这个命令会产生一个大的数据文件，取决与你采集的进程与CPU的配置，如果一台服务器有16个 CPU，每秒抽样99次，持续30秒，就得到 47,520 个调用栈，长达几十万甚至上百万行。上面的命令中，perf record表示记录，-F 99表示每秒99次，-p 25633是进程号，即对哪个进程进行分析，-g表示记录调用栈，sleep 30则是持续30秒，参数信息可以视情况调整。生成的数据采集文件在当前目录下，名称为perf.data。\nperf record命令可以从高到低排列统计每个调用栈出现的百分比，显示结果如下图所示:\nroot@master:~# sudo perf report -n --stdio 这样的效果对使用者来说还是不那么直观易读，这时候，火焰🔥图也就真正的派上用途了。\n制作火焰🔥图 火焰🔥图并非一定就是火焰系列的颜色主题，只是通过🔥色系更能表达出含义。火焰图常见的类型有 On-CPU, Off-CPU, 还有 Memory, Hot/Cold, [Differential](http://www.brendangregg.com/blog/2014-11-09/differential-flame-graphs.html \u0026quot;Differential\u0026quot;) 等等. on-CPU/off-cpu`的区别就是一个是用于CPU是性能瓶颈，一个是IO是性能瓶颈，当你不知道当前的服务器的性能瓶颈究竟是什么的时候，你可以使用这两种类型进行对比，通过两种火焰图的差别是比较大的，如果两张火焰图长得差不多, 那么通常认为CPU被其它进程抢占了.\n另外一种情况就是如果无法确定当前的系统瓶颈, 可以通过压测工具来确认 : 通过压测工具看看能否让CPU使用率趋于饱和, 如果能那么使用 On-CPU 火焰图, 如果不管怎么压, CPU 使用率始终上不来, 那么多半说明程序被 IO 或锁卡住了, 此时适合使用 Off-CPU 火焰图. 你可以通过压测工具进行测试，目前比较常用的就是ab和wrk，我建议尝试使用诸如 wrk 之类更现代的压测工具.\n 如果选择 ab 的话, 那么务必记得开启 -k 选项, 以避免耗尽系统的可用端口\n Github上有Brendan D. Gregg 的 Flame Graph 工程实现了一套生成火焰图的脚本.我们可以直接克隆下来直接用。\ncd \u0026amp;\u0026amp; git clone https://github.com/brendangregg/FlameGraph.git 生成火焰🔥图，我们一般都遵循以下流程\n 捕获堆栈: 使用perf捕捉进程运行堆栈信息 折叠堆栈: 对抓取的系统和程序运行每一时刻的堆栈信息进行分析组合, 将重复的堆栈累计在一起, 从而体现出负载和关键路径，通过stackcollapse脚本完成 生成火焰图：分析 stackcollapse 输出的堆栈信息渲染成火焰图  Flame Graph中提供了抓取不同信息的脚本，可以按需使用。下面我们需要对捕获到的进程堆栈信息perf.data进行折叠，生成折叠的堆栈信息:\nroot@master:~# perf script -i /root/perf.data \u0026amp;\u0026gt; /root/perf.unfold 用 stackcollapse-perf.pl 将 perf 解析出的内容 perf.unfold 中的符号进行折叠\nroot@master:~/FlameGraph# ls aix-perf.pl docs example-perf.svg pkgsplit-perf.pl stackcollapse-aix.pl stackcollapse-go.pl stackcollapse-ljp.awk stackcollapse-pmc.pl stackcollapse-vsprof.pl test.sh demos example-dtrace-stacks.txt files.pl range-perf.pl stackcollapse-bpftrace.pl stackcollapse-instruments.pl stackcollapse-perf.pl stackcollapse-recursive.pl stackcollapse-vtune.pl dev example-dtrace.svg flamegraph.pl README.md stackcollapse-elfutils.pl stackcollapse-java-exceptions.pl stackcollapse-perf-sched.awk stackcollapse-sample.awk stackcollapse-xdebug.php difffolded.pl example-perf-stacks.txt.gz jmaps record-test.sh stackcollapse-gdb.pl stackcollapse-jstack.pl stackcollapse.pl stackcollapse-stap.pl test root@master:~/FlameGraph# ./stackcollapse-perf.pl /root/perf.unfold \u0026amp;\u0026gt; /root/perf.folded root@master:~/FlameGraph# 最后就是生成火焰🔥图了\nroot@master:~/FlameGraph# ./flamegraph.pl /root/perf.folded \u0026gt; /root/perf.svg 当然也可以通过管道符|将整个过程简化:\ncd \u0026amp;\u0026amp; perf script | FlameGraph/stackcollapse-perf.pl | FlameGraph/flamegraph.pl \u0026gt; process.svg 最后在谷歌浏览器上打开该火焰图:\n火焰图是基于stack信息生成的SVG 图片, 用来展示 CPU 的调用栈。\n  y 轴表示调用栈, 每一层都是一个函数. 调用栈越深, 火焰就越高, 顶部就是正在执行的函数, 下方都是它的父函数.\n  x 轴表示抽样数, 如果一个函数在 x 轴占据的宽度越宽, 就表示它被抽到的次数多, 即执行的时间长. 注意, x 轴不代表时间, 而是所有的调用栈合并后, 按字母顺序排列的.\n  火焰图就是看顶层的哪个函数占据的宽度最大. 只要有\u0026quot;平顶\u0026quot;(plateaus), 就表示该函数可能存在性能问题。颜色没有特殊含义, 因为火焰图表示的是 CPU 的繁忙程度, 所以一般选择暖色调.\n当调用栈不完整调用栈过深时，某些系统只返回前面的一部分（比如前10层）;当函数名缺失，函数没有名字，编译器只用内存地址来表示（比如匿名函数），所以使用火焰图也是存在分析不到的地方。你也可以通过以下脚本进行采集分析火焰图:\nif [ $# -ne 1 ];then echo \u0026#34;Usage: $0seconds\u0026#34; exit 1 fi perf record -a -g -o perf.data \u0026amp; PID=`ps aux| grep \u0026#34;perf record\u0026#34;| grep -v grep| awk \u0026#39;{print $2}\u0026#39;` if [ -n \u0026#34;$PID\u0026#34; ]; then sleep $1 kill -s INT $PID fi # wait until perf exite sleep 1 perf script -i perf.data \u0026amp;\u0026gt; perf.unfold perl stackcollapse-perf.pl perf.unfold \u0026amp;\u0026gt; perf.folded perl flamegraph.pl perf.folded \u0026gt;perf.svg ","id":14,"section":"posts","summary":"文章背景 日常的工作中，会收到一堆CPU使用率过高的告警邮件，遇到某台服务的CPU被占满了，这时候我们就要去查看是什么进程将服务器的CPU资源","tags":["运维技术","内核调优"],"title":"如何确定CPU消耗在哪些函数上","uri":"https://www.devopsman.cn/2020/07/04/","year":"2020"},{"content":"Lazydocker的使用背景 平时的工作管理中会使用到各种各样的命令行工具，有些人是比较厌烦的去学习各种命令以及选项，比如Docker的各种命令和选项其实都让人很头大，于是就有人做出来一款名为Lazydocker的专为懒人设计的Docker和docker-compose终端管理工具，该工具使用Go语言开发基于gocui实现。\n如果你发现自己的项目出了问题，或者是服务down掉了，那么Lazydocker就可以立刻给你提供帮助。Lazydocker可以帮助我们DEBUG自己的项目或者服务，并且在出现问题时立刻重启所有组件，然后给我们提供详细的日志流。其中，日志流还会进行细项分类，并允许我们了解特定服务中发生的所有事情。是github上一个比较🔥的开源工具。\n除此之外，想要记住所有的Docker命令其实是很麻烦的，而且跨多个终端窗口跟踪容器也几乎是无法做到的。但是在Lazydocker的帮助下，我们就可以在一个终端窗口中查看到所有你所需要的信息，而且常用的命令仅需按下一个键即可实现。毫无疑问，Lazydocker绝对是懒人们的福音!我们先来看一下效果图：\nLazydocker的功能 现在让我们先了解一下Lazydocker的功能\n  清晰的查看所有的Docker和Docker-compose容器环境的状态\n  实时查看容器/服务日志；\n  查看容器指标的ascii图，这样您不仅可以感觉到而且看起来像开发人员\n  自定义这些图形以测量您想要的几乎任何指标\n  进入容器/服务；\n  重启/移除/重建容器或服务；\n  查看给定镜像的历史层信息\n  修改占用磁盘空间的容器、镜像或卷；\n  Lazydocker安装配置 你可以直接在Github上下载二进制文件，也可以通过容器运行该命令，此处我直接使用二进制命令\nwget https://github.com/jesseduffield/lazydocker/releases/download/v0.9/lazydocker_0.9_Darwin_x86_64.tar.gz tar xf lazydocker_0.9_Darwin_x86_64.tar.gz cp lazydocker /usr/local/bin/ \u0026amp;\u0026amp; chmod +x /usr/local/bin/lazydocker 因为该命令太长了，所以建议配置一个命令别名，方便我们使用:\necho \u0026#34;alias lzd=\u0026#39;lazydocker\u0026#39;\u0026#34; \u0026gt;\u0026gt; ~/.zshrc source ~/.zshrc 此时，直接执行lzd即可在终端上显示容器的各种指标状态图\nLazydocker的配置 Lazydocker支持用户自定义配置，对于不同的操作系统其配置文件存在不同的位置上\n OSX: ~/Library/Application Support/jesseduffield/lazydocker/config.yml Linux: ~/.config/jesseduffield/lazydocker/config.yml Windows: C:\\\\Users\\\\\u0026lt;User\u0026gt;\\\\AppData\\\\Roaming\\\\jesseduffield\\\\lazydocker\\\\config.yml  不过你可以在打开Lazydocker之后，鼠标移到左上方，使用快捷键o即可打开配置文件进入编辑状态，此时直接编辑即可，想要知道每个配置文件选项的含义，可以参考开发配置参数查看，下面是一个配置的案例：\ngui: scrollHeight: 2 theme: activeBorderColor: - green - bold inactiveBorderColor: - white optionsTextColor: - blue returnImmediately: false wrapMainPanel: false reporting: undetermined commandTemplates: dockerCompose: docker-compose restartService: \u0026#39;{{ .DockerCompose }} restart {{ .Service.Name }}\u0026#39; stopService: \u0026#39;{{ .DockerCompose }} stop {{ .Service.Name }}\u0026#39; serviceLogs: \u0026#39;{{ .DockerCompose }} logs --since=60m --follow {{ .Service.Name }}\u0026#39; viewServiceLogs: \u0026#39;{{ .DockerCompose }} logs --follow {{ .Service.Name }}\u0026#39; rebuildService: \u0026#39;{{ .DockerCompose }} up -d --build {{ .Service.Name }}\u0026#39; recreateService: \u0026#39;{{ .DockerCompose }} up -d --force-recreate {{ .Service.Name }}\u0026#39; viewContainerLogs: docker logs --timestamps --follow --since=60m {{ .Container.ID }} containerLogs: docker logs --timestamps --follow --since=60m {{ .Container.ID }} allLogs: \u0026#39;{{ .DockerCompose }} logs --tail=300 --follow\u0026#39; viewAlLogs: \u0026#39;{{ .DockerCompose }} logs\u0026#39; dockerComposeConfig: \u0026#39;{{ .DockerCompose }} config\u0026#39; checkDockerComposeConfig: \u0026#39;{{ .DockerCompose }} config --quiet\u0026#39; serviceTop: \u0026#39;{{ .DockerCompose }} top {{ .Service.Name }}\u0026#39; customCommands: containers: - name: bash attach: true command: docker exec -it {{ .Container.ID }} /bin/sh serviceNames: [] oS: openCommand: open {{filename}} openLinkCommand: open {{link}} update: dockerRefreshInterval: 100ms stats: graphs: - caption: CPU (%) statPath: DerivedStats.CPUPercentage color: blue - caption: Memory (%) statPath: DerivedStats.MemoryPercentage color: green Lazydocker的快捷键 在lazydocker的交互式界面中，还提供了多种快捷键，大家可以通过快捷键快速的在多种功能之间切换。\nProject Containers Services Images Volumes Main 更多精彩专辑  运维架构专辑: 包含一些运维技术的使用经验分享、技术架构案例、学习交流等 kubernetes实践案例:都是kubernetes相关的实践案例，毫无套路，大家都说好! DevOps实践案例: 专辑内包括Jenkins和Gitlab各自持续集成、持续发布的实践案例，以及各种模板库的最佳实践。  ","id":15,"section":"posts","summary":"Lazydocker的使用背景 平时的工作管理中会使用到各种各样的命令行工具，有些人是比较厌烦的去学习各种命令以及选项，比如Docker的各种","tags":["docker","容器","运维"],"title":"Docker容器命令行可视化工具-Lazydocker","uri":"https://www.devopsman.cn/2020/06/10/","year":"2020"},{"content":"安装 Apache HTTP 服务 Apache是世界使用排名第一的Web服务器软件。它可以运行在几乎所有广泛使用的计算机平台上，由于其跨平台和安全性被广泛使用，是最流行的Web服务器端软件之一。\n 执行如下命令，安装Apache服务及其扩展包。  yum -y install httpd httpd-manual mod_ssl mod_perl mod_auth_mysql 返回类似如下图结果则表示安装成功。\n执行如下命令，启动Apache服务。  systemctl start httpd.service 测试Apache服务是否安装并启动成功。  Apache默认监听80端口，所以只需在浏览器访问ECS分配的IP地址http://\u0026lt;ECS公网地址\u0026gt;，如下图：\n安装 MySQL 数据库 由于使用wordpress搭建云上博客，需要使用MySQL数据库存储数据，所以这一步我们安装一下MySQL。\n 执行如下命令，下载并安装MySQL官方的Yum Repository。  wget http://dev.mysql.com/get/mysql57-community-release-el7-10.noarch.rpm yum -y install mysql57-community-release-el7-10.noarch.rpm yum -y install mysql-community-server 执行如下命令，启动 MySQL 数据库。  systemctl start mysqld.service 执行如下命令，查看MySQL运行状态。  systemctl status mysqld.service 执行如下命令，查看MySQL初始密码。  grep \u0026quot;password\u0026quot; /var/log/mysqld.log 执行如下命令，登录数据库。  mysql -uroot -p 执行如下命令，修改MySQL默认密码。   说明 新密码设置的时候如果设置的过于简单会报错，必须同时包含大小写英文字母、数字和特殊符号中的三类字符。\n ALTER USER 'root'@'localhost' IDENTIFIED BY 'NewPassWord1.'; 执行如下命令，创建wordpress库。  create database wordpress; 执行如下命令，创建wordpress库。 执行如下命令，查看是否创建成功。  show databases; 输入exit退出数据库。  安装 PHP 语言环境 WordPress是使用PHP语言开发的博客平台，用户可以在支持PHP和MySQL数据库的服务器上架设属于自己的网站。也可以把WordPress当作一个内容管理系统（CMS）来使用。\n 执行如下命令，安装PHP环境。  yum -y install php php-mysql gd php-gd gd-devel php-xml php-common php-mbstring php-ldap php-pear php-xmlrpc php-imap 执行如下命令创建PHP测试页面。  echo \u0026#34;\u0026lt;?php phpinfo(); ?\u0026gt;\u0026#34; \u0026gt; /var/www/html/phpinfo.php 执行如下命令，重启Apache服务。  systemctl restart httpd 打开浏览器，访问http://\u0026lt;ECS公网地址\u0026gt;/phpinfo.php，显示如下页面表示PHP语言环境安装成功。  Wordpress安装和配置 本小节将在已搭建好的LAMP 环境中，安装部署 WordPress\n 执行如下命令，安装wordpress。  yum -y install wordpress 显示如下信息表示安装成功。\n修改WordPress配置文件。  1）执行如下命令，修改wp-config.php指向路径为绝对路径。\n# 进入/usr/share/wordpress目录。 cd /usr/share/wordpress # 修改路径。 ln -snf /etc/wordpress/wp-config.php wp-config.php # 查看修改后的目录结构。 ll 2）执行如下命令，移动wordpress到Apache根目录。\n# 在Apache的根目录/var/www/html下，创建一个wp-blog文件夹。 mkdir /var/www/html/wp-blog mv * /var/www/html/wp-blog/ 3）执行以下命令修改wp-config.php配置文件。\n在执行命令前，请先替换以下三个参数值。\n database_name_here为之前步骤中创建的数据库名称，本示例为wordpress。 username_here为数据库的用户名，本示例为root。 password_here为数据库的登录密码，本示例为NewPassWord1.。  sed -i \u0026#39;s/database_name_here/wordpress/\u0026#39; /var/www/html/wp-blog/wp-config.php sed -i \u0026#39;s/username_here/root/\u0026#39; /var/www/html/wp-blog/wp-config.php sed -i \u0026#39;s/password_here/NewPassWord1./\u0026#39; /var/www/html/wp-blog/wp-config.php 4）执行以下命令，查看配置文件信息是否修改成功。\ncat -n /var/www/html/wp-blog/wp-config.php 执行如下命令，重启Apache服务。  systemctl restart httpd ","id":16,"section":"posts","summary":"安装 Apache HTTP 服务 Apache是世界使用排名第一的Web服务器软件。它可以运行在几乎所有广泛使用的计算机平台上，由于其跨平台和安全性被广泛使用，是","tags":["博客","运维"],"title":"ECS上手把手搭建个人博客系统wordpress","uri":"https://www.devopsman.cn/2020/06/10/","year":"2020"},{"content":"最近有很多朋友看了我的文章之后，问我你终端是怎么设置的，为什么如此炫酷，这这这\u0026hellip;让我怎么说，难道我的文章不干吗？还是特干看不下去了？好吧，今天趁着周末给大家分享一下，如何设置一个你认为很高大上的终端，对于常用终端的发烧友来说无疑是一篇值得收藏的好文章，哈哈\n想要配置这么高大上的终端？你需要安装一个叫做Iterm的远程连接工具，官方说是macOS Terminal Replacement，对的，你的电脑必须是Macos系统才可以，接下来我们一步一步的教你如何配置一个令你心仪的iterm，首先我们需要掌握一下基础的东西，然后在自定义一些喜欢的东西。\n前期准备 安装Iterm2 在官网下载好iterm2的二进制文件，然后直接托放到Macos系统的Application的文件夹内，然后你就可以在启动台``launch里面找到Iterm了。安装完成之后，打开软件，以下的操作均在iterm2上操作`。\n配置oh-my-zsh 现在我们就需要配置一个神助工具oh-my-zsh来让你的Iterm2起飞，最开始的时候，你的iterm是这样的\n下面，我们修改默认的bash为zsh，这里要注意的是，后期需要做一些命令别名,环境变量的时候，就不再是以前的bashrc等了，应是~/.zshrc或者/etc/zshrc啦。\nbrew install zsh # 安装zsh chsh -s /bin/zsh 如果你想要修改回来\nchsh -s /bin/bash 修改之后，我们通过wget或者curl的方式下载oh-my-zsh，以下安装方式选择任意一种即可:\n curl方式  $ sh -c \u0026#34;$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\u0026#34;  wget方式  $ sh -c \u0026#34;$(wget https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh -O -)\u0026#34; 执行完之后，就如下所示:\n实际上，就是通过curl或者wget的方式将oh-my-zsh的代码仓库克隆到你的家目录下/Users/marionxue/.oh-my-zsh下\n提供字体支持 接下来我们需要安装一些字体，这些字体提供了某些oh-my-zsh主题的特殊字符的支持，在美化配置上是必不可少的。\ngit clone https://github.com/powerline/fonts.git --depth=1 # clone git clone https://github.com/powerline/fonts.git --depth=1 # install cd fonts ./install.sh # clean-up a bit cd .. rm -rf fonts 安装好之后，选择一款Powerline字体了：iterm2 -\u0026gt; Preferences -\u0026gt; Profiles -\u0026gt; Text -\u0026gt; Font -\u0026gt; Change Font（我用的是nerd-fonts因为该字体支持非 ASCII 码字体，如下图所示）,当你的终端在配置主题之后，出现了乱码，那么你的字体一定是没有选对，这里需要使用powerline系列的字体，这是需要注意的地方。如果你需要安装nerd-fonts,直接使用brew即可\nbrew tap caskroom/fonts brew cask install font-hack-nerd-font 美化iterm2 一些特别吸引眼球的美化设置都是通过在主题的基础上自定义修改实现的，默认的oh-my-zsh使用的主题是ZSH_THEME=\u0026quot;robbyrussell\u0026quot;，个人并不好看，下面我们自己选择一个合适的主题，我们可以在/Users/marionxue/.oh-my-zsh/themes下面查看默认提供的主题。而配置文件就是我们之前提及到的~/.zshrc文件，\n我们打开该文件找到ZSH_THEME=\u0026quot;robbyrussell\u0026quot;，然后修改robbyrussell为你喜欢的主题即可，我这里使用的是ZSH_THEME=\u0026quot;agnoster\u0026quot;不过我也推荐这个主题。记得每次修改~/.zshrc文件之后，需要source ~/.zshrc让配置文件生效，我们看一下效果图：\n但是看起来比较单调，不是那么的高大上啊，于是有些人在网上肯定发现过以下这种样式，看起来相对比较完美，研究一下下面这种图的做法：\n仔细观察，命令提示符左侧显示的是git的分支，后侧显示的执行结果状态、执行命令的数量以及时间，这是一个比较流行的第三方 PowerLevel9k 开源主题，我们将其下载到~/.oh-my-zsh/custom/themes下，详细的配置设置参考github上的文档:\ncd ~/.oh-my-zsh/custom/themes https://github.com/bhilburn/powerlevel9k.git 修改配置文件中的主题设置为:\nZSH_THEME=\u0026#34;powerlevel9k/powerlevel9k\u0026#34; 然后配置git,history和time，我们在\nPOWERLEVEL9K_LEFT_PROMPT_ELEMENTS=(context dir vcs) POWERLEVEL9K_RIGHT_PROMPT_ELEMENTS=(status history time) 这样就完成了上图的配置。当然你可以设置一些带有icon的模式比如:\nPOWERLEVEL9K_MODE=\u0026#39;nerdfont-complete\u0026#39; 效果如下:\n你也可以自已定义一些背景颜色，你可以通过执行一下命令获取配色\nspectrum_bls # 显示前配色 spectrum_ls # 显示后配色 美化进阶 这里就是美化终端的基本操作，下面我们看一下如何在原有的基础上配置一下自己喜欢的icon和插件：\n红色框 红色框是iterm的资源状态栏，你可以在iterm2中使用快捷键command+,或者菜单栏打开首选项，进行设置，后面就不在赘述如何带开了，\n点击configure status bar将需要的托放在下面既可\n青色的🐳 青色的🐳是一个emoji.可以利用touchbar填充上去，可以写文字\n黄色的目录 这里是安装的一些高效易用的插件\n git-open: 一个快捷工具，当你cd到一个版本化的代码仓库目录下，执行git-open会自动打开浏览器并跳转到该代码仓库地址 zsh-autosuggestions:这是一个自动提示之前执行过的命令历史 zsh-syntax-highlighting:这是一个zsh语法高亮的插件  这些插件安装配置简单，只需要clone对应的代码仓库到/Users/marionxue/.oh-my-zsh/custom/plugins下，然后在.zshrc配置文件下的plugins处配置上即可，注意我这里使用的是agnoster主题。\ncd ~/.oh-my-zsh/custom/plugins git clone https://github.com/paulirish/git-open git clone https://github.com/zsh-users/zsh-autosuggestions git clone https://github.com/zsh-users/zsh-syntax-highlighting vim ~/.zshrc plugins=( git zsh-syntax-highlighting zsh-autosuggestions git-open ) 最后是配置生效\nsource ~/.zshrc 蓝色框内的配置 这里是通过agnoster主题加上一些自己修改的主题实现的。然后结合~/.zshrc配置命令行提示符的显示，你可以在emoji网站1，emoji网站2上找到对应的Bytes\n其中的devcluster和kube-ops是我的k8s集群的集群名以及对应的命名空间，这里是利用kubectx、kube-ps1实现的，具体的你可以参考，下面是我的部分配置文件\nZSH_THEME=\u0026#34;agnoster\u0026#34; DISABLE_AUTO_UPDATE=\u0026#34;true\u0026#34; DISABLE_UPDATE_PROMPT=\u0026#34;true\u0026#34; plugins=( git zsh-syntax-highlighting zsh-autosuggestions kubectl kube-ps1 git-open ) KUBE_PS1_SYMBOL_ENABLE=false KUBE_PS1_PREFIX=\u0026#34;\\u2638\\uFE0F \u0026#34; KUBE_PS1_SUFFIX=\u0026#34; \u0026#34; KUBE_PS1_DIVIDER=\u0026#34;\\xf0\\x9f\\x94\\xa5 \u0026#34; PS1=\u0026#39;$(kube_ps1)\u0026#39;$PS1 好了，到这里就结束了，希望你能有所收获~\n","id":17,"section":"posts","summary":"最近有很多朋友看了我的文章之后，问我你终端是怎么设置的，为什么如此炫酷，这这这\u0026hellip;让我怎么说，难道我的文章不干吗？还是特干看不下","tags":["运维"],"title":"Mac下手把手带你打造骚气的命令行终端","uri":"https://www.devopsman.cn/2020/06/10/","year":"2020"},{"content":"P\u0008ython脚本分析拉钩招聘网站职位 场景 最近发现一些朋友想要跳槽，正值疫情，也不知道现在市场的如何，同时目前的IT行业更是越来越难,技术革新越来越快，对新的岗位的需求也是不断的变化，因此就会想知道现在的应聘岗位对面试者的要求有哪些，各地的某个岗位薪资范围大概是多少等信息时候，我们就需要到某个招聘网站上不断的刷页面，看数据，但是简单的想一下，可以通过Python脚本来批量的分析招聘网站上各个岗位在不同城市的需求，高效的快捷的方便我们掌握大致的方向。\n实现 如何获取数据，需要掌握基本的Python爬虫知识，[requests](https://requests.readthedocs.io/en/master/ Requests)模块就可以搞定了，在爬取数据之后，将其存在Excel中，因此需要[xlwt](https://xlwt.readthedocs.io/en/latest/ xlwt)模块处理，当然在诸多的Python模块中，你可以选择你喜欢的，毕竟能抓老鼠的猫都是好猫。\nxlwt 1.3.0 requests 2.18.4 下面我们就拿拉钩网站为例，思考和获取部分的数据作为个人简单的分析参考，脚本中没有涉及到隐私数据信息，大可放心，同时也是为了找工作的小伙伴们提供一下参考的方向：\n注意：\n 脚本中获取的是通过指定的页的数量获取全国各城市的岗位信息，你可以修改FetchData方法中的referer和请求地址中城市的值，以便获取你需要的目标城市的岗位信息 如果获取的比较频繁的话，可能会出现以下情况，这里你可以通过设置代理的方式解决，免费的代理IP网站上有很多，你可以参考这篇获取代理的文章。  在交互式输入需要获取的页数之后，爬取的数据将会存储在当前执行位置下的data.xls。  下面就简单的提供一下写好的Python脚本：\n#!/usr/bin/env python3.4 # encoding: utf-8 \u0026#34;\u0026#34;\u0026#34; Created on 2020-06-26 @title: \u0026#39;爬去网站的招聘信息\u0026#39; @author: marionxue \u0026#34;\u0026#34;\u0026#34; import requests import xlwt # 获取存储职位信息的json对象，遍历获得公司名、福利待遇、工作地点、学历要求、工作类型、发布时间、职位名称、薪资、工作年限 def FetchData(url, datas): my_headers = { \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36\u0026#34;, \u0026#34;Referer\u0026#34;: \u0026#34;https://www.lagou.com/jobs/list_Python?city=%E5%85%A8%E5%9B%BD\u0026amp;cl=false\u0026amp;fromSearch=true\u0026amp;labelWords=\u0026amp;suginput=\u0026#34;, \u0026#34;Content-Type\u0026#34;: \u0026#34;application/x-www-form-urlencoded;charset = UTF-8\u0026#34; } ses = requests.session() # 获取session ses.headers.update(my_headers) # 更新头部信息 ses.get(\u0026#34;https://www.lagou.com/jobs/list_%E9%83%91%E5%B7%9Ejava?city=%E5%85%A8%E5%9B%BD\u0026amp;cl=false\u0026amp;fromSearch=true\u0026amp;labelWords=\u0026amp;suginput=\u0026#34;) content = ses.post(url=url, data=datas) result = content.json() info = result[\u0026#39;content\u0026#39;][\u0026#39;positionResult\u0026#39;][\u0026#39;result\u0026#39;] info_list = [] for job in info: information = [] information.append(job[\u0026#39;positionId\u0026#39;]) # 岗位对应ID information.append(job[\u0026#39;city\u0026#39;]) # 岗位对应城市 information.append(job[\u0026#39;companyFullName\u0026#39;]) # 公司全名 information.append(job[\u0026#39;companyLabelList\u0026#39;]) # 福利待遇 information.append(job[\u0026#39;district\u0026#39;]) # 工作地点 information.append(job[\u0026#39;education\u0026#39;]) # 学历要求 information.append(job[\u0026#39;firstType\u0026#39;]) # 工作类型 information.append(job[\u0026#39;formatCreateTime\u0026#39;]) # 发布时间 information.append(job[\u0026#39;positionName\u0026#39;]) # 职位名称 information.append(job[\u0026#39;salary\u0026#39;]) # 薪资 information.append(job[\u0026#39;workYear\u0026#39;]) # 工作年限 info_list.append(information) return info_list def main(): page = int(input(\u0026#39;请输入你要抓取的页码总数：\u0026#39;)) info_result = [] title = [\u0026#39;岗位id\u0026#39;, \u0026#39;城市\u0026#39;, \u0026#39;公司全名\u0026#39;, \u0026#39;福利待遇\u0026#39;, \u0026#39;工作地点\u0026#39;, \u0026#39;学历要求\u0026#39;, \u0026#39;工作类型\u0026#39;, \u0026#39;发布时间\u0026#39;, \u0026#39;职位名称\u0026#39;, \u0026#39;薪资\u0026#39;, \u0026#39;工作年限\u0026#39;] info_result.append(title) for x in range(1, page + 1): url = \u0026#39;https://www.lagou.com/jobs/positionAjax.json?needAddtionalResult=false\u0026#39; datas = { \u0026#39;first\u0026#39;: \u0026#39;false\u0026#39;, \u0026#39;pn\u0026#39;: x, \u0026#39;kd\u0026#39;: \u0026#39;devops工程师\u0026#39;, } try: info = FetchData(url, datas) info_result = info_result + info print(\u0026#34;第%s页数据已采集\u0026#34; % x) except Exception as msg: print(\u0026#34;第%s页数据采集出现问题\u0026#34; % x) # 创建workbook,即excel workbook = xlwt.Workbook(encoding=\u0026#39;utf-8\u0026#39;) # 创建表,第二参数用于确认同一个cell单元是否可以重设值 worksheet = workbook.add_sheet(datas[\u0026#34;kd\u0026#34;], cell_overwrite_ok=True) for i, row in enumerate(info_result): # print(row) for j, col in enumerate(row): worksheet.write(i, j, col) workbook.save(\u0026#39;data.xls\u0026#39;) if __name__ == \u0026#39;__main__\u0026#39;: main() 当我们需要查看某个岗位的时候，我们只需要在58行处修改岗位的名称即可，然后输入你要采集多少页的数据即可，这样很快就会将数据采集并且存储在Excel表中\n数据显示 数据基本上完成采集，当然对于自己有需要的话，还可以继续完善啊\n","id":18,"section":"posts","summary":"P\u0008ython脚本分析拉钩招聘网站职位 场景 最近发现一些朋友想要跳槽，正值疫情，也不知道现在市场的如何，同时目前的IT行业更是越来越难,技术革","tags":["Python","编程"],"title":"python脚本分析拉钩网站招聘信息","uri":"https://www.devopsman.cn/2020/06/10/","year":"2020"},{"content":"​ 最近发现很多的微信文章上出现了一种logo设计，跟P站的logo设计风格一样让人印象深刻，黑底白字，配上一小撮橙色，给人极强的冲击力。后来查了一下在Github上发现有一个有意思的程序员弄的一个在线创意Logo生成器，利用Logoly Pro可以生成类似YouTobe、P站等Logo风格的标志。分享给大家，于是就有了今天这篇文章。\nlogoly的由来 这个风格的制作是由tencent的一位大牛开发制作的，叫做白宦成,这里是他对于这个网页版工具的自述：\n我自己平时经常要做一些 Side Project ，在做 Side Project 的时候，就涉及到了要做 Logo ，但是作为一个没有设计感的程序员，在做 Logo 时总是会做出一些很丑的 Logo ，于是痛定思痛，想想有没有什么有用的工具可以帮助我生成好看的 Logo。对于我来说，也不需要太过复杂，能够满足我自己的要求就行。\nP站Logo风格图片素材制作的官方网站：https://logoly.pro/ 代码仓库里面是这样描述的，”一个简单的在线徽标生成器，适合想要轻松设计徽标的人们\u0026rdquo;。\nlogoly的功能和特色  生成P站风格的logo 支持导出png格式的图片 支持修改前缀/后缀/背景的颜色 支持设置字体的大小 支持自定义字体TODO，可能是为了防止侵权，避免使用一些未开源的字体  logoly的使用方法  打开logoly网站: https://logoly.pro/s 在文本框内输入需要的文本 修改你需要的颜色和字体的大小 点击导出即可下载png格式的图片 还是不太懂得，可以看一下哔哩哔哩B站上别人发的文章，或者就放弃使用吧！  ","id":19,"section":"posts","summary":"​ 最近发现很多的微信文章上出现了一种logo设计，跟P站的logo设计风格一样让人印象深刻，黑底白字，配上一小撮橙色，给人极强的冲击力。后来","tags":["工具类"],"title":"P站风格Logo生成器","uri":"https://www.devopsman.cn/2020/06/07/","year":"2020"},{"content":"了解过Traefik,nginx-ingress的同学都知道他们的官方文档都是基于mkdocs和material主题制作而成，你觉得这种文档库怎么样？有没有心动把自己的文档也整成那样的？，下面我们直接动手干起来吧。\n配置pip国内的下载源\n☸️ devcluster🔥 kube-ops ~  🐳 👉 cat ./.pip/pip.conf [global] index-url = https://pypi.tuna.tsinghua.edu.cn/simple 安装mkdocs\n☸️ devcluster🔥 kube-ops ~  🐳 👉 pip3 install mkdocs Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple Requirement already satisfied: mkdocs in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (1.1.2) ... WARNING: You are using pip version 19.2.3, however version 20.1.1 is available. You should consider upgrading via the \u0026#39;pip install --upgrade pip\u0026#39; command. 创建并运行一个新的文档库\n☸️ devcluster🔥 kube-ops ~  🐳 👉 mkdocs new cloudnativecosystem ☸️ devcluster🔥 kube-ops ~  🐳 👉 cd cloudnativecosystem ☸️ devcluster🔥 kube-ops ~/cloudnativecosystem  🐳 👉 ls docs mkdocs.yml 创建一个新的GIT仓库https://github.com/linuxermaster/cloudnativecosystem_mkdocs\n配置通过SSH无密访问代码仓库\nssh-keygen -t rsa -C \u0026#34;email@qq.com\u0026#34; 然后将公钥拷贝到GITHUB的SSH-KEY中\n测试本地是否可以远程连接\n# ssh -T git@github.com Hi linuxermaster! You\u0026#39;ve successfully authenticated, but GitHub does not provide shell access. 然后需要在cloudnativecosystem目录内初始化.git仓库，然后添加远程仓库地址\ncd cloudnativecosystem git init git remote add origin https://github.com/linuxermaster/cloudnativecosystem_mkdocs.git mkdocs build --clean # 创建编译后的静态页面以及样式文件 mkdocs gh-deploy --clean # push到gh-deploy分支下 此时，即可通过https://linuxermaster.github.io/cloudnativecosystem_mkdocs/访问\nhttpstat https://linuxermaster.github.io/cloudnativecosystem_mkdocs/ 现在开始优化mkdocs以及配置主题样式了，这部分工作都在mkdocs.yml中完成\nsite_name: 云原生生态圈 nav: - 主页: \u0026#34;index.md\u0026#34; - 关于: \u0026#34;about.md\u0026#34; - 自动化: \u0026#34;devops.md\u0026#34; theme: material repo_url: https://github.com/linuxermaster/cloudnativecosystem_mkdocs.git repo_name: \u0026#34;Opening on Github\u0026#34; site_description: \u0026#34;这是一个mkdocs的demo测试知识库网站\u0026#34; site_author: \u0026#34;Marionxue\u0026#34; copyright: \u0026#34;1994 - 2020\u0026#34; 其中theme指定的是你的主题，这个主题就是我们常见到的traefik,nginx-ingress等在使用的官方文档的主题，如果使用它，你可能还需要额外的安装一下\npip3 install mkdocs-material 安装之后，执行mkdocs gh-deploy --clean即可访问网站的主题:\n每次手动部署都是比较麻烦的，于是我们利用永久免费的travis来帮助解决这个问题:\nlanguage: python # Set the build language to Python python: 3.6 # Set the version of Python to use branches: master # Set the branch to build from install: - pip install mkdocs mkdocs-material pymdown-extensions pygments # Install the required dependencies script: true # Skip script (Don\u0026#39;t use this if one already exists) before_deploy: - mkdocs build --verbose --clean --strict # Build a local version of the docs deploy: # Deploy documentation to Github in the gh_pages branch provider: pages skip_cleanup: true github_token: $GITHUB_API_KEY local_dir: site on: branch: master 然后我们使用GITHUB的账号登录travis, 地址是:https://travis-ci.org，在登录进去之后，我们选择合适的启用CICD\n然后点击setting进入仓库的配置界面，设置以下三个环境变量\n其中,GITHUB_API_KEY是从gitub上获取的Access Token,剩下的两个是用户名和密码。完成之后，我们就可以手动的触发以下构建:\n紧接着，我们为了方便我们知道构建是否完成，我们在README.md文件上添加了一个构建的状态展示:\n复制RESULT信息到readme.md中，即可显示每次构建的状态信息:\n查看一下我们的构建历史\n","id":20,"section":"posts","summary":"了解过Traefik,nginx-ingress的同学都知道他们的官方文档都是基于mkdocs和material主题制作而成，你觉得这种文档","tags":["devops","运维"],"title":"mkdocs自动化部署知识库","uri":"https://www.devopsman.cn/2020/06/04/","year":"2020"},{"content":"autojump autojump 是一款非常方便的命令行下的目录跳转工具，它能帮你快速从目录访问的历史记录中统计出各个目录的访问频次和权重，这样，就能方便的让你在各个目录中迅速跳转了。只要你记得之前某个访问过的目录的大概名字，配合 autojump，就能快速的跳转过去，再也不用打一长串的 cd 命令了。使用方式如下：\n安装 brew install autojump # 大多数mac系统都会安装zsh,因此当安装配置好autojump之后，需要将以下代码写入到~/.zshrc [[ -s `brew --prefix`/etc/autojump.sh ]] \u0026amp;\u0026amp; . `brew --prefix`/etc/autojump.sh # 最后在source一遍 source ~/.zshrc tig Git 已经成为我们平时经常用到的版本控制管理工具。通常，我们用 git log 命令来查看 git 提交的历史记录。如果你已经厌倦了 git log 那种千篇一律的界面，那么 tig 绝对是一个不可错过的命令行下查看 git 历史提交记录的工具\ntig 的界面看起来比起 git log 要酷炫不少，而且使用起来也挺方便。此外，tig 的默认按键绑定还跟 Vim 比较类似，真是 Vimer 的福音。\nbrew install tig git summary 平时我们在多人合作开发一个项目的时候，想要大致了解一下每个人对这个项目提交的 commit 数量和大致的贡献度，那么 git summary 这个命令绝对能满足你的要求\ngit summary 通过对项目中每一个 commit 的统计，能大致计算出每个 contributor 的提交次数和贡献百分比。让你对这个项目的贡献度能一目了然。\n默认 git 是不会带有这个命令的，需要在 Mac OS 下额外安装一个扩展包：\nbrew install git-extras the_silver_searcher UNIX/Linux 系统有不少好用的工具，用于文本搜索的 grep 或许是其中最常用的工具之一。尽管平时称心如意，在面对数百万行的代码库时，grep 的用户体验实在堪忧。还好，我们有 the silver searcher （即 ag）这样迅捷的替代品\n  在文本中搜索指定的字符串,显示包含字符串的行\nag \u0026#34;password\u0026#34; ./tagret.file   显示含有指定字符串的文件名\nag -l \u0026#39;password\u0026#39; .   tmux 当我们在远程执行命令的时候，经常性的会因为网络异常打嗝造成回话断开，命令执行失败，此时你就需要一个 Tmux，它是一个终端复用器（terminal multiplexer)。这样即使你远程连接服务器的笔记本断网了，也不会终端你在服务器上执行的命令；\n 它允许在单个窗口中，同时访问多个会话。这对于同时运行多个命令行程序很有用 它可以让新窗口\u0026quot;接入\u0026quot;已经存在的会话 它允许每个会话有多个连接窗口，因此可以多人实时共享会话。 它还支持窗口任意的垂直和水平拆分。  jq json 文件处理以及格式化显示，支持高亮，可以替换 python -m json.tool\nhttpstat HTTP 响应的可视化命令行工具,请求含有http/https前缀的 url,支持所有curl支持的除了-w,-D,-o,-S,-s之外的所有选项。\nthefuck 命令行打错了以后，打一个fuck就会自动纠正。\nshellcheck ShellCheck，用于 Shell 脚本的静态分析工具，在网页上检查你的脚本：https://www.shellcheck.net/ ,shellcheck 具体会检查一些什么问题呢，以下给出一个不完整的问题检查列表\n 引号问题 条件判断 ShellCheck 可以识别大多数不正确的条件判断语句 常见的对命令的错误使用 ShellCheck 识别很多初学者的语法错误 ShellCheck 可以提出一些风格改进建议 ShellCheck 可以识别一些数据和拼写错误 ShellCheck 可以做出一些增强脚本鲁棒性的建议 ShellCheck 警告你使用了 shebang 不支持的特性. ShellCheck 可以识别到一些其他问题  glances top/htop`的替代方案，官网地址:https://nicolargo.github.io/glances/\n","id":21,"section":"posts","summary":"","tags":["运维"],"title":"技术人员最常用的命令行工具","uri":"https://www.devopsman.cn/2020/05/30/","year":"2020"},{"content":"一般情况下Kubernetes可以通过ReplicaSet以一个Pod模板创建多个Pod副本，但是它们都是无状态的，任何时候它们都可以被一个全新的Pod替换。然而有状态的Pod需要另外的方案确保当一个有状态的Pod挂掉后，这个Pod实例需要在别的节点上重建，但是新的实例必须与被替换的实例拥有相同的名称、网络标识和状态。这就是StatefulSet管理Pod的手段。\n对于容器集群，有状态服务的挑战在于，通常集群中的任何节点都并非100%可靠的，服务所需的资源也会动态地更新改变。当节点由于故障或服务由于需要更多的资源而无法继续运行在原有节点上时，集群管理系统会为该服务重新分配一个新的运行位置，从而确保从整体上看，集群对外的服务不会中断。若采用本地存储，当服务漂移后数据并不会随着服务转移到新的节点，重启服务就会出现数据丢失的困境。\n本文目的是通过一个MySQL的主从集群搭建，深入了解Kubernetes的StatfulSet管理。为了降低实验的外部依赖，存储层面上，我采用的是本地存储，当然生产上不建议这样做，生产环境的存储推荐官方介绍到的的GCE、NFS、Ceph等存储方案，因为这些方案支持动态供给的特性，允许开发人员通过PVC的定义，快速实现数据有效存储，所以你绝不应该把一个宿主机上的目录当作PV使用，只是本文用于实验需要，采用Local Persistent Volume的手段，目的只是为了验证StatefulSet的状态管理功能。\n Kubernetes Master Kubernetes Node （测试演示，所有的副本都会在其上运行） Kubernetes DNS服务已开启  实验目的  搭建一个主从复制（Master-Slave）的MySQL集群 从节点可以水平扩展 所有的写操作只能在MySQL主节点上执行 读操作可以在MySQL主从节点上执行 从节点能同步主节点的数据  本地存储原理 为了快速搭建测试环境，我们这里使用了本地存储，也就是说，用户希望Kubernetes能够直接使用宿主机上的本地磁盘目录，而不依赖于远程存储服务，来提供持久化的容器Volume。不过这里有个难点：\n 我们把存储固定在一个节点上，但是Pod在调度的时候，是飘来飘去的，怎么能让Pod通过PVC也能固定在PVC上？ 给这个Pod加上一个nodeAffinity行不行？  当然行，但是这变相破坏了开发人员对资源对象的定义规范了，开发人员应该不需要时刻考虑调度的细节。调度的改动应该交给运维就行。所以我们为了实现本地存储，我们采用了延迟绑定的方法。方法很简单，我们都知道StorageClass一般由运维人员设计，我们只需要在StorageClass指定no-provisioner。这是因为Local Persistent Volume目前尚不支持Dynamic Provisioning，所以它没办法在用户创建PVC的时候，就自动创建出对应的PV。与此同时，这个StorageClass还定义了一个volumeBindingMode=WaitForFirstConsumer的属性。它是Local Persistent Volume里一个非常重要的特性，即：延迟绑定。\nkind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: local-storage provisioner: kubernetes.io/no-provisioner volumeBindingMode: WaitForFirstConsumer 实验步骤 创建PV 先在Node （实验用的Node节点IP是node1）节点上，预先分配几个PV（不建议在生产上这样操作），要保证在node1上有对应的目录\nssh root@node1 mkdir -pv /data/svr/projects/{mysql,mysql2,mysql3} 01-persistentVolume-1.yaml\napiVersion: v1 kind: PersistentVolume metadata: name: example-mysql-pv spec: capacity: storage: 15Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Delete storageClassName: local-storage local: path: /data/svr/projects/mysql nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - node1 01-persistentVolume-2.yaml\napiVersion: v1 kind: PersistentVolume metadata: name: example-mysql-pv-2 spec: capacity: storage: 15Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Delete storageClassName: local-storage local: path: /data/svr/projects/mysql2 nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - node1 01-persistentVolume-3.yaml\napiVersion: v1 kind: PersistentVolume metadata: name: example-mysql-pv-3 spec: capacity: storage: 15Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Delete storageClassName: local-storage local: path: /data/svr/projects/mysql3 nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - node1 记住，这是在生产上不推荐的做法，我只是实验用途才这样手动预先创建，**正规的做法应该通过StorageClass采用Dynamic Provisioning， 而不是Static Provisioning机制生产PV。\nkubectl apply -f 01-persistentVolume-{1..3}.yaml persistentvolume/example-mysql-pv1 created persistentvolume/example-mysql-pv2 created persistentvolume/example-mysql-pv3 created 创建StorageClass 02-storageclass.yaml\nkind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: local-storage provisioner: kubernetes.io/no-provisioner volumeBindingMode: WaitForFirstConsumer 执行创建：\nkubectl apply -f 02-storageclass.yaml storageclass.storage.k8s.io/local-storage created 创建Namespace 03-mysql-namespace.yaml\napiVersion: v1 kind: Namespace metadata: name: mysql labels: app: mysql 执行创建：\nkubectl apply -f 03-mysql-namespace.yaml namespace/mysql created 创建数据库的配置文件configmap 使用ConfigMap为Master/Slave节点分配不同的配置文件\n04-mysql-configmap.yaml\napiVersion: v1 kind: ConfigMap metadata: name: mysql namespace: mysql labels: app: mysql data: master.cnf: | # Master配置 [mysqld] log-bin=mysqllog skip-name-resolve slave.cnf: | # Slave配置 [mysqld] super-read-only skip-name-resolve log-bin=mysql-bin replicate-ignore-db=mysql 创建执行：\nkubectl apply -f 04-mysql-configmap.yaml configmap/mysql created 创建MySQL密码Secret 05-mysql-secret.yaml\napiVersion: v1 kind: Secret metadata: name: mysql-secret namespace: mysql labels: app: mysql type: Opaque data: password: MTIzNDU2 # echo -n \u0026#34;123456\u0026#34; | base64 创建执行：\nkubectl apply -f 05-mysql-secret.yaml secret/mysql-secret created 使用Service为MySQL提供读写分离 06-mysql-services.yaml\napiVersion: v1 kind: Service metadata: name: mysql namespace: mysql labels: app: mysql spec: ports: - name: mysql port: 3306 clusterIP: None selector: app: mysql --- apiVersion: v1 kind: Service metadata: name: mysql-read namespace: mysql labels: app: mysql spec: ports: - name: mysql port: 3306 selector: app: mysql   用户所有写请求，必须以DNS记录的方式直接访问到Master节点，也就是mysql-0.mysql这条DNS记录。\n  用户所有读请求，必须访问自动分配的DNS记录可以被转发到任意一个Master或Slave节点上，也就是mysql-read这条DNS记录。\n  kubectl apply -f 06-mysql-services.yaml $ kubectl get svc -n mysql NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE mysql ClusterIP None \u0026lt;none\u0026gt; 3306/TCP 20s mysql-read ClusterIP 10.0.0.63 \u0026lt;none\u0026gt; 3306/TCP 20s 创建MySQL集群实例 使用StatefulSet搭建MySQL主从集群\n07-mysql-statefulset.yaml\napiVersion: apps/v1 kind: StatefulSet metadata: name: mysql namespace: mysql labels: app: mysql spec: selector: matchLabels: app: mysql serviceName: mysql replicas: 2 template: metadata: labels: app: mysql spec: initContainers: - name: init-mysql image: mysql:5.7 env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql-secret key: password command: - bash - \u0026#34;-c\u0026#34; - | set -ex # 从Pod的序号，生成server-id [[ $(hostname) =~ -([0-9]+)$ ]] || exit 1 ordinal=${BASH_REMATCH[1]} echo [mysqld] \u0026gt; /mnt/conf.d/server-id.cnf # 由于server-id不能为0，因此给ID加100来避开它 echo server-id=$((100 + $ordinal)) \u0026gt;\u0026gt; /mnt/conf.d/server-id.cnf # 如果Pod的序号为0，说明它是Master节点，从ConfigMap里把Master的配置文件拷贝到/mnt/conf.d目录下 # 否则，拷贝ConfigMap里的Slave的配置文件 if [[ ${ordinal} -eq 0 ]]; then cp /mnt/config-map/master.cnf /mnt/conf.d else cp /mnt/config-map/slave.cnf /mnt/conf.d fi volumeMounts: - name: conf mountPath: /mnt/conf.d - name: config-map mountPath: /mnt/config-map - name: clone-mysql image: gcr.io/google-samples/xtrabackup:1.0 env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql-secret key: password command: - bash - \u0026#34;-c\u0026#34; - | set -ex # 拷贝操作只需要在第一次启动时进行，所以数据已经存在则跳过 [[ -d /var/lib/mysql/mysql ]] \u0026amp;\u0026amp; exit 0 # Master 节点（序号为 0）不需要这个操作 [[ $(hostname) =~ -([0-9]+)$ ]] || exit 1 ordinal=${BASH_REMATCH[1]} [[ $ordinal == 0 ]] \u0026amp;\u0026amp; exit 0 # 使用ncat指令，远程地从前一个节点拷贝数据到本地 ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql # 执行 --prepare，这样拷贝来的数据就可以用作恢复了 xtrabackup --prepare --target-dir=/var/lib/mysql volumeMounts: - name: data mountPath: /var/lib/mysql subPath: mysql - name: conf mountPath: /etc/mysql/conf.d containers: - name: mysql image: mysql:5.7 env: # - name: MYSQL_ALLOW_EMPTY_PASSWORD # value: \u0026#34;1\u0026#34; - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql-secret key: password ports: - name: mysql containerPort: 3306 volumeMounts: - name: data mountPath: /var/lib/mysql subPath: mysql - name: conf mountPath: /etc/mysql/conf.d resources: requests: cpu: 500m memory: 1Gi livenessProbe: exec: command: [\u0026#34;mysqladmin\u0026#34;, \u0026#34;ping\u0026#34;, \u0026#34;-uroot\u0026#34;, \u0026#34;-p${MYSQL_ROOT_PASSWORD}\u0026#34;] initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 readinessProbe: exec: command: [\u0026#34;mysqladmin\u0026#34;, \u0026#34;ping\u0026#34;, \u0026#34;-uroot\u0026#34;, \u0026#34;-p${MYSQL_ROOT_PASSWORD}\u0026#34;] initialDelaySeconds: 5 periodSeconds: 2 timeoutSeconds: 1 - name: xtrabackup image: gcr.io/google-samples/xtrabackup:1.0 ports: - name: xtrabackup containerPort: 3307 env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql-secret key: password command: - bash - \u0026#34;-c\u0026#34; - | set -ex cd /var/lib/mysql # 从备份信息文件里读取MASTER_LOG_FILE和MASTER_LOG_POS这2个字段的值，用来拼装集群初始化SQL if [[ -f xtrabackup_slave_info ]]; then # 如果xtrabackup_slave_info文件存在，说明这个备份数据来自于另一个Slave节点 # 这种情况下，XtraBackup工具在备份的时候，就已经在这个文件里自动生成了“CHANGE MASTER TO”SQL语句 # 所以，只需要把这个文件重命名为change_master_to.sql.in，后面直接使用即可 mv xtrabackup_slave_info change_master_to.sql.in # 所以，也就用不着xtrabackup_binlog_info了 rm -f xtrabackup_binlog_info elif [[ -f xtrabackup_binlog_info ]]; then # 如果只是存在xtrabackup_binlog_info文件，说明备份来自于Master节点，就需要解析这个备份信息文件，读取所需的两个字段的值 [[ $(cat xtrabackup_binlog_info) =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1 rm xtrabackup_binlog_info # 把两个字段的值拼装成SQL，写入change_master_to.sql.in文件 echo \u0026#34;CHANGE MASTER TO MASTER_LOG_FILE=\u0026#39;${BASH_REMATCH[1]}\u0026#39;,\\ MASTER_LOG_POS=${BASH_REMATCH[2]}\u0026#34; \u0026gt; change_master_to.sql.in fi # 如果存在change_master_to.sql.in，就意味着需要做集群初始化工作 if [[ -f change_master_to.sql.in ]]; then # 但一定要先等MySQL容器启动之后才能进行下一步连接MySQL的操作 echo \u0026#34;Waiting for mysqld to be ready（accepting connections）\u0026#34; until mysql -h 127.0.0.1 -uroot -p${MYSQL_ROOT_PASSWORD} -e \u0026#34;SELECT 1\u0026#34;; do sleep 1; done echo \u0026#34;Initializing replication from clone position\u0026#34; # 将文件change_master_to.sql.in改个名字 # 防止这个Container重启的时候，因为又找到了change_master_to.sql.in，从而重复执行一遍初始化流程 mv change_master_to.sql.in change_master_to.sql.orig # 使用change_master_to.sql.orig的内容，也就是前面拼装的SQL，组成一个完整的初始化和启动Slave的SQL语句 mysql -h 127.0.0.1 -uroot -p${MYSQL_ROOT_PASSWORD} \u0026lt;\u0026lt; EOF $(\u0026lt; change_master_to.sql.orig), MASTER_HOST=\u0026#39;mysql-0.mysql.mysql\u0026#39;, MASTER_USER=\u0026#39;root\u0026#39;, MASTER_PASSWORD=\u0026#39;${MYSQL_ROOT_PASSWORD}\u0026#39;, MASTER_CONNECT_RETRY=10; START SLAVE; EOF fi # 使用ncat监听3307端口。 # 它的作用是，在收到传输请求的时候，直接执行xtrabackup --backup命令，备份MySQL的数据并发送给请求者 exec ncat --listen --keep-open --send-only --max-conns=1 3307 -c \\ \u0026#34;xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1 --user=root --password=${MYSQL_ROOT_PASSWORD}\u0026#34; volumeMounts: - name: data mountPath: /var/lib/mysql subPath: mysql - name: conf mountPath: /etc/mysql/conf.d volumes: - name: conf emptyDir: {} - name: config-map configMap: name: mysql volumeClaimTemplates: - metadata: name: data spec: accessModes: - \u0026#34;ReadWriteOnce\u0026#34; storageClassName: local-storage resources: requests: storage: 3Gi 整体的StatefulSet有两个Replicas，一个Master，一个Slave，然后使用init-mysql这个initContainers进行配置文件的初始化。接着使用clone-mysql这个initContainers进行数据的传输；同时使用xtrabackup这个sidecar容器进行SQL初始化和数据传输功能。\n创建 StatefulSet：\nkubectl apply -f 07-mysql-statefulset.yaml 可以看到，StatefulSet启动成功后，会有两个Pod运行。接下来，我们可以尝试向这个MySQL集群发起请求，执行一些SQL操作来验证它是否正常。整个过程因为拉取mysql和一个gcr.io/google-samples/xtrabackup:1.0国外的镜像会很慢,但是在创建mysql-0拉取一次之后，后续创建mysql-1就相对很快了。\n最后，容器检查pod的运行状态\n服务验证  验证主从状态：  kubectl -n mysql exec mysql-1 -c mysql -- bash -c \u0026#34;mysql -uroot -p123456 -e \u0026#39;show slave status \\G\u0026#39;\u0026#34; mysql: [Warning] Using a password on the command line interface can be insecure. *************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: mysql-0.mysql.mysql Master_User: root Master_Port: 3306 Connect_Retry: 10 Master_Log_File: mysqllog.000003 Read_Master_Log_Pos: 154 Relay_Log_File: mysql-1-relay-bin.000002 Relay_Log_Pos: 319 Relay_Master_Log_File: mysqllog.000003 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: mysql Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 154 Relay_Log_Space: 528 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0 Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 100 Master_UUID: 1bad4d64-6290-11ea-8376-0242ac113802 Master_Info_File: /var/lib/mysql/master.info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: Executed_Gtid_Set: Auto_Position: 0 Replicate_Rewrite_DB: Channel_Name: Master_TLS_Version: 接下来，我们通过Master容器创建数据库和表、插入数据库。  kubectl -n mysql exec mysql-0 -c mysql -- bash -c \u0026#34;mysql -uroot -p123456 -e \u0026#39;create database test’\u0026#34; kubectl -n mysql exec mysql-0 -c mysql -- bash -c \u0026#34;mysql -uroot -p123456 -e \u0026#39;use test;create table counter(c int);’\u0026#34; kubectl -n mysql exec mysql-0 -c mysql -- bash -c \u0026#34;mysql -uroot -p123456 -e \u0026#39;use test;insert into counter values(123)’\u0026#34; 然后，我们观察Slave节点是否都同步到数据了。  kubectl -n mysql exec mysql-1 -c mysql -- bash -c \u0026#34;mysql -uroot -p123456 -e \u0026#39;use test;select * from counter’\u0026#34; c 123 当看到输出结果，主从同步正常了。\n扩展从节点 在有了StatefulSet以后，你就可以像Deployment那样，非常方便地扩展这个MySQL集群，比如：\nkubectl -n mysql scale statefulset mysql -—replicas=3 $ kubectl get po -n mysql NAME READY STATUS RESTARTS AGE mysql-0 2/2 Running 0 22m mysql-1 2/2 Running 0 22m mysql-2 2/2 Running 0 20s 这时候，一个新的mysql-2就创建出来了，我们继续验证新扩容的节点是否都同步到主节点的数据。\nkubectl -n mysql exec mysql-2 -c mysql -- bash -c \u0026#34;mysql -uroot -p123456 -e \u0026#39;use test;select * from counter’\u0026#34; c 123 当看到输出结果，主从同步正常了。也就是说从StatefulSet为我们新创建的mysql-2上，同样可以读取到之前插入的记录。也就是说，我们的数据备份和恢复，都是有效的。\n","id":22,"section":"posts","summary":"一般情况下Kubernetes可以通过ReplicaSet以一个Pod模板创建多个Pod副本，但是它们都是无状态的，任何时候它们都可以被一个","tags":null,"title":"","uri":"https://www.devopsman.cn/1/01/01/","year":"0001"},{"content":"常用的 ping，tracert，nslookup 一般用来判断主机的网络连通性，其实 Linux 下有一个更好用的网络联通性判断工具，它可以结合ping nslookup tracert 来判断网络的相关特性，这个命令就是 mtr。mtr 全称 my traceroute，是一个把 ping 和 traceroute 合并到一个程序的网络诊断工具。\ntraceroute默认使用UDP数据包探测，而mtr默认使用ICMP报文探测，ICMP在某些路由节点的优先级要比其他数据包低，所以测试得到的数据可能低于实际情况。\n安装方法 1.Windows系统可以直接在https://cdn.ipip.net/17mon/besttrace.exe下载BestTrace工具并安装。也可以在https://github.com/oott123/WinMTR/releases GitHub上下载MTR专用工具，该工具为免安装，下载后可以直接使用。\n2.Linux可以直接运行命令进行安装。\nDebian/Ubuntu 系统 sudo apt install mtr RedHat/CentOS 系统 sudo yum install mtr 3.Apple客户端可以在App store搜索Best NetTools下载安装\n4.Android客户端：可以在Google Play上下载TracePing，但是由于国内Google Play无法访问，笔者自行下载下来，可以直接访问 https://dwz.cn/KCdNPH4c 下载TracePing。\n使用 MTR使用非常简单，查看本机到 qq.com 的路由以及连接情况直接运行如下命令：\nmtr qq.com MTR qq.com 测试界面\n具体输出的参数含义为：\n 第一列是IP地址 丢包率：Loss 已发送的包数：Snt 最后一个包的延时：Last 平均延时：Avg 最低延时：Best 最差延时：Wrst 方差（稳定性）：StDev  参数说明 -r or \u0026ndash;report 使用 mtr -r qq.com 来打印报告，如果不使用 -r or \u0026ndash;report 参数 mtr 会不断动态运行。使用 report 选项， mtr 会向 qq.com 主机发送 10 个 ICMP 包，然后直接输出结果。通常情况下 mtr 需要几秒钟时间来输出报告。mtr 报告由一系列跳数组成，每一跳意味着数据包通过节点或者路由器来达到目的主机。\n一般情况下 mtr 前几跳都是本地 ISP，后几跳属于服务商比如 腾讯数据中心，中间跳数则是中间节点，如果发现前几跳异常，需要联系本地 ISP 服务提供上，相反如果后几跳出现问题，则需要联系服务提供商，中间几跳出现问题，则需要联系运营商进行处理\n默认使用 -r 参数来生成报告，只会发送10个数据包，如果想要自定义数据包数量，可以使用 -c 参数\n-s or \u0026ndash;packetsize 使用 -s 来指定ping数据包的大小\nmtr -s 100 qq.com 100 bytes 数据包会用来发送，测试，如果设置为负数，则每一次发送的数据包的大小都会是一个随机数。\n-c 指定发送数量\nmtr -c 100 qq.com -n 不进行主机解释\n使用 -n 选项来让 mtr 只输出 IP，而不对主机 host name 进行解释\nmtr -n qq.com\nMTR结果分析 当我们分析 MTR 报告时候，最好找出每一跳的任何问题。除了可以查看两个服务器之间的路径之外，MTR 在它的七列数据中提供了很多有价值的数据统计报告。 Loss% 列展示了数据包在每一跳的丢失率。 Snt 列记录的多少个数据包被送出。 使用 –report 参数默认会送出10个数据包。如果使用 –report-cycles=[number-of-packets] 选项，MTR 就会按照 [number-of-packets] 指定的数量发出 ICMP 数据包。\nLast, Avg, Best 和 Wrst 列都标识数据包往返的时间，使用的是毫秒（ ms ）单位表示。 Last 表示最后一个数据包所用的时间， Avg 表示评价时间， Best 和 Wrst 表示最小和最大时间。在大多数情况下，平均时间（ Avg）列需要我们特别注意。\n最后一列 StDev 提供了数据包在每个主机的标准偏差。如果标准偏差越高，说明数据包在这个节点的延时越不相同。标准偏差会让您了解到平均延时是否是真的延时时间的中心点，或者测量数据受到某些问题的干扰。\n例如，如果标准偏差很大，说明数据包的延迟是不确定的。一些数据包延迟很小（例如：25ms），另一些数据包延迟很大（例如：350ms）。当10个数据包全部发出后，得到的平均延迟可能是正常的，但是平均延迟是不能很好的反应实际情况的。如果标准偏差很高，使用最好和最坏的延迟来确定平均延迟是一个较好的方案。\n在大多数情况下，您可以把 MTR 的输出分成三大块。根据配置，第二或第三跳一般都是您的本地 ISP，倒数第二或第三跳一般为您目的主机的ISP。中间的节点是数据包经过的路由器。\n当分析 MTR 的输出时，您需要注意两点： loss 和 latency。\n网络丢包 如果在任何一跳上看到 loss 的百分比，这就说明这一跳上可能有问题了。当然，很多服务提供商人为限制 ICMP 发送的速率，这也会导致此问题。那么如何才能指定是人为的限制 ICMP 传输 还是确定有丢包的现象？此时需要查看下一跳。如果下一跳没有丢包现象，说明上一条是人为限制的。如下示例：\n人为限制MTR丢包\n在此例中，第4跳发生了丢包现象，但是接下来几条都没任何丢包现象，说明第二跳的丢包是人为限制的。如果在接下来的几条中都有丢包，那就可能是第二跳有问题了。请记住，ICMP 包的速率限制和丢失可能会同时发生。\nMTR丢包截图\n从上面的图中，您可以看从第13跳和第17跳都有 10% 的丢包率，从接下来的几跳都有丢包现象，但是最后15,16跳都是100%的丢包率，我们可以猜测到100%的丢包率除了网络糟糕的原因之前还有人为限制 ICMP。所以，当我们看到不同的丢包率时，通常要以最后几跳为准。\n还有很多时候问题是在数据包返回途中发生的。数据包可以成功的到达目的主机，但是返回过程中遇到“困难”了。所以，当问题发生后，我们通常需要收集反方向的 MTR 报告。\n此外，互联网设施的维护或短暂的网络拥挤可能会带来短暂的丢包率，当出现短暂的10%丢包率时候，不必担心，应用层的程序会弥补这点损失。\n网络延迟 除了可以通过MTR报告查看丢包率，我们也还可以看到本地到目的之间的时延。因为是不通的位置，延迟通常会随着条数的增加而增加。所以，延迟通常取决于节点之间的物理距离和线路质量。\nMTR查看网络延迟\n从上面的MTR报告截图中，我们可以看到从第11跳到12跳的延迟猛增，直接导致了后面的延迟也很大，一般有可能是11跳到12跳属于不通地域，物理距离导致时延猛增，也有可能是第12条的路由器配置不当，或者是线路拥塞。需要具体问题进行具体的分析。\n然而，高延迟并不一定意味着当前路由器有问题。延迟很大的原因也有可能是在返回过程中引发的。从这份报告的截图看不到返回的路径，返回的路径可能是完全不同的线路，所以一般需要进行双向MTR测试。\n注：ICMP 速率限制也可能会增加延迟，但是一般可以查看最后一条的时间延迟来判断是否是上述情况。\n根据MTR结果解决网络问题 MTR 报告显示的路由问题大都是暂时性的。很多问题在24小时内都被解决了。大多数情况下，如果您发现了路由问题，ISP 提供商已经监视到并且正在解决中了。当您经历网络问题后，可以选择提醒您的 ISP 提供商。当联系您的提供商时，需要发送一下 MTR 报告和相关的数据。没有有用的数据，提供商是没有办法去解决问题的。\n然而大多数情况下，路由问题是比较少见的。比较常见的是因为物理距离太长，或者上网高峰，导致网络变的很慢。尤其是跨越大西洋和太平洋的时候，网络有时候会变的很慢。这种情况下，建议就近接入客户的节点。\n如果在腾讯云上有网络的问题，且不能解读MTR的报告，可以直接联系腾讯云相关售后人员进行处理。\n","id":23,"section":"posts","summary":"常用的 ping，tracert，nslookup 一般用来判断主机的网络连通性，其实 Linux 下有一个更好用的网络联通性判断工具，它可以结合ping nslookup","tags":null,"title":"","uri":"https://www.devopsman.cn/1/01/01/","year":"0001"},{"content":"wordpress简介 WordPress是一个免费的开源内容管理系统CMS建立在具有PHP处理功能的MYSQL数据库上。凭借其可扩展的插件体系结构和模板系统，以及其大部分管理都可以通过Web界面完成的事实，当创建从博客到产品页面再到电子商务网站的各种类型的网站时，WordPress是一种流行的选择。\n运行WordPress通常涉及安装LAMP（Linux，Apache，MySQL和PHP）或LEMP（Linux，Nginx，MySQL和PHP）堆栈，这可能很耗时。但是，通过使用诸如Docker和Docker Compose之类的工具，您可以简化设置首选堆栈和安装WordPress的过程。您可以使用images来标准化诸如库，配置文件和环境变量之类的图像，而不是手动安装单个组件，并在容器中运行这些图像，在共享操作系统上运行的隔离进程。此外，通过使用Compose，您可以协调多个容器（例如，一个应用程序和数据库）以相互通信。\n在本教程中，您将构建多容器WordPress安装。您的容器将包括一个MySQL数据库，一个Nginx Web服务器和WordPress本身。您还可以通过“加密”获取要与站点关联的域的TLS / SSL证书来保护安装。最后，您将设置一个cron工作来续订证书，以便您的域保持安全。\n先决条件 要遵循本教程，您将需要：\n 安装Docker以及Docker-compose git  步骤1 —定义Web服务器配置 在运行任何容器之前，我们的第一步将是为Nginx Web服务器定义配置。我们的配置文件将包括一些WordPress特定的位置块，以及将“加密”验证请求定向到Certbot客户端以进行自动证书更新的位置块。\n首先，为您的WordPress设置创建一个名为的项目目录，wordpress并导航至该目录：\nmkdir wordpress \u0026amp;\u0026amp; cd wordpress 接下来，为配置文件创建目录：\nmkdir nginx-conf 在此文件中，我们将添加一个服务器块，其中包含用于我们的服务器名称和文档根的指令，以及用于指示Certbot客户端对证书，PHP处理和静态资产请求的指令的位置块。\n将以下代码粘贴到文件中。确保kubemaster.top用您自己的域名替换：\ntouch nginx-conf/nginx.conf 然后编辑配置文件:\nserver { listen 80; server_name kubemaster.top www.kubemaster.top; index index.php index.html index.htm; root /var/www/html; location ~ /.well-known/acme-challenge { allow all; root /var/www/html; } location / { try_files $uri $uri/ /index.php$is_args$args; } location ~ \\.php$ { try_files $uri =404; fastcgi_split_path_info ^(.+\\.php)(/.+)$; fastcgi_pass wordpress:9000; fastcgi_index index.php; include fastcgi_params; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_param PATH_INFO $fastcgi_path_info; } location ~ /\\.ht { deny all; } location = /favicon.ico { log_not_found off; access_log off; } location = /robots.txt { log_not_found off; access_log off; allow all; } location ~* \\.(css|gif|ico|jpeg|jpg|js|png)$ { expires max; log_not_found off; } } 我们的服务器块包含以下信息：\n指令：\n listen：这告诉Nginx监听port 80，这将允许我们使用Certbot的webroot插件来进行证书请求。请注意，我们尚未包括端口443-成功获得证书后，我们将更新配置以包括SSL。 server_name：这定义了您的服务器名称和用于请求服务器的服务器块。确保kubemaster.top在此行用您自己的域名替换。 index：index伪指令定义了在处理对服务器的请求时将用作索引的文件。我们在此处修改了默认的优先级顺序，将其移到了优先级的index.php前面，index.html以便Nginxindex.php在可能的情况下对调用的文件进行优先级排序。 root：我们的root指令将根目录命名为对我们服务器的请求。这个目录中，/var/www/html被作为挂载点的创建由我们的指示在构建时的WordPress Dockerfile。这些Dockerfile指令还确保将WordPress版本中的文件安装到该卷上。  定位块：\n location ~ /.well-known/acme-challenge：此位置块将处理对.well-known目录的请求，Certbot将在该目录中放置一个临时文件，以验证我们域的DNS可以解析到我们的服务器。有了此配置后，我们将能够使用Certbot的webroot插件来获取我们域的证书。 location /：在此位置块中，我们将使用try_files指令检查与单个URI请求匹配的文件。Not Found但是，我们不会使用默认参数返回404状态，而是将控制权传递给WordPress的index.php文件。 location ~ \\.php$：此位置块将处理PHP处理并将这些请求代理到我们的wordpress容器。因为我们的WordPress Docker映像将基于该php:fpm映像，所以我们还将在此块中包含特定于FastCGI协议的配置选项。Nginx需要一个独立的PHP处理器来处理PHP请求：在我们的示例中，这些请求将由映像php-fpm随附的处理器处理php:fpm。此外，此位置块包括特定于FastCGI的指令，变量和选项，这些指令会将请求代理到运行在我们wordpress容器中的WordPress应用程序，设置已解析请求URI的首选索引以及解析URI请求。 location ~ /\\.ht：此块将处理.htaccess文件，因为Nginx无法提供文件。该deny_all指令确保.htaccess文件永远不会提供给用户。 location = /favicon.ico，location = /robots.txt：这些块确保对/favicon.ico和的请求/robots.txt都不会被记录。 location ~* \\.(css|gif|ico|jpeg|jpg|js|png)$：此块关闭了静态资产请求的日志记录，并确保这些资产具有很高的可缓存性，因为它们的服务成本通常很高。  准备Docker-compose配置清单 version: \u0026#39;3\u0026#39; services: db: image: mysql:8.0 container_name: db restart: unless-stopped env_file: .env environment: - MYSQL_DATABASE=wordpress volumes: - dbdata:/var/lib/mysql command: \u0026#39;--default-authentication-plugin=mysql_native_password\u0026#39; networks: - app-network wordpress: depends_on: - db image: wordpress:5.2.1-fpm-alpine container_name: wordpress restart: unless-stopped env_file: .env environment: - WORDPRESS_DB_HOST=db:3306 - WORDPRESS_DB_USER=$MYSQL_USER - WORDPRESS_DB_PASSWORD=$MYSQL_PASSWORD - WORDPRESS_DB_NAME=wordpress volumes: - wordpress:/var/www/html networks: - app-network webserver: depends_on: - wordpress image: nginx:1.15.12-alpine container_name: webserver restart: unless-stopped ports: - \u0026#34;80:80\u0026#34; - \u0026#34;443:443\u0026#34; volumes: - /webserver/nginx18/config:/etc/nginx - wordpress:/var/www/html - /webserver/nginx18/data:/usr/share/nginx - certbot-etc:/etc/letsencrypt networks: - app-network volumes: certbot-etc: wordpress: dbdata: networks: app-network: driver: bridge 以上部署了wordpress、mysql、nginx三个服务，这三个服务的数据均是通过volume提供，因此我们需要对这些目录进行备份，分别为:\n wordpress dbdata  [root@10-255-20-45 wordpress]# docker inspect wordpress_wordpress [ { \u0026#34;CreatedAt\u0026#34;: \u0026#34;2021-05-08T16:18:45+08:00\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;Labels\u0026#34;: { \u0026#34;com.docker.compose.project\u0026#34;: \u0026#34;wordpress\u0026#34;, \u0026#34;com.docker.compose.version\u0026#34;: \u0026#34;1.26.2\u0026#34;, \u0026#34;com.docker.compose.volume\u0026#34;: \u0026#34;wordpress\u0026#34; }, \u0026#34;Mountpoint\u0026#34;: \u0026#34;/var/lib/docker/volumes/wordpress_wordpress/_data\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;wordpress_wordpress\u0026#34;, \u0026#34;Options\u0026#34;: null, \u0026#34;Scope\u0026#34;: \u0026#34;local\u0026#34; } ] [root@10-255-20-45 wordpress]# docker inspect wordpress_dbdata [ { \u0026#34;CreatedAt\u0026#34;: \u0026#34;2021-05-08T16:12:46+08:00\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;Labels\u0026#34;: { \u0026#34;com.docker.compose.project\u0026#34;: \u0026#34;wordpress\u0026#34;, \u0026#34;com.docker.compose.version\u0026#34;: \u0026#34;1.26.2\u0026#34;, \u0026#34;com.docker.compose.volume\u0026#34;: \u0026#34;dbdata\u0026#34; }, \u0026#34;Mountpoint\u0026#34;: \u0026#34;/var/lib/docker/volumes/wordpress_dbdata/_data\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;wordpress_dbdata\u0026#34;, \u0026#34;Options\u0026#34;: null, \u0026#34;Scope\u0026#34;: \u0026#34;local\u0026#34; } ] 然后将wordpress的代码数据提交到git\ncd /var/lib/docker/volumes/wordpress_wordpress/_data git init git remote add origin https://github.com/marionxue/wp.git git add . git submodule add https://github.com/LuRenJiasWorld/wp-settings-api-class.git wp-content/plugins/wp-editormd/vendor/lurenjiasworld/wp-settings-api-class git commit -am \u0026#34;init wordpress site\u0026#34; git config --global user.email \u0026#34;marionxue@qq.com\u0026#34; git config --global user.name \u0026#34;marionxue\u0026#34; ","id":24,"section":"posts","summary":"wordpress简介 WordPress是一个免费的开源内容管理系统CMS建立在具有PHP处理功能的MYSQL数据库上。凭借其可扩展的插件体","tags":["云原生"],"title":"wordPress原来还可以这么用","uri":"https://www.devopsman.cn/1/01/01/","year":"0001"},{"content":"前言 最近和一同学聊天，他想换工作，然后去面了一家大厂。当时，他在简历上写着精通TCP/IP，本着对TCP协议稍有了解，面试官也不会深问的想法，就写了精通二字。没想到，大意了\n开场 朋友约的是十点半的面试，提前了十分钟到，然后安静地坐在沙发等待，顺便回忆下之前看的资料。快到十点半时，一个高瘦，穿着格子衫的男子推开门而进，说了句“你好，我们来开始面试吧！”，朋友不失礼貌地笑着回了句“行”\n面试官：看你简历说精通TCP和IP，那我们来讨论下网络模型和TCP、IP协议，讲下你的理解先  朋友（怎么一上来就问TCP，不按套路出牌啊，不该问问java基础吗？不过常规题，我还行）  朋友：网络模型一般分七层：应用层、表示层、会话层、传输层、网络层、数据链路层、物理层。应用层的协议包括HTTP、FTP、SMTP，而TCP属于传输层，IP协议则属于网络层 朋友：TCP/IP网络模型层次由上到下，层层包装，每一层都对应不同的协议解析，我来画个图   面试官：看你画的图，TCP有自己的首部结构，这都有哪些字段，最好说说它们的作用  朋友（什么鬼！当我百度词典，这怎么记得住？等等，昨天晚上好像看过，有印象）  朋友：继续画个图，直观点  朋友：TCP首部结构先是16位的源端口号和目标端口号、接着是32位的序列号和确认号。再下面就是4bit的头部长度和6个bit的保留位及6bit的标志位 朋友：16位的属性则有窗口大小（控制发送窗口），检验和（校验数据段是否未被修改）及紧急指针。最后是选项，其长度由头部长度决定 朋友：详细说下序列号，它是TCP报文段的一数字编号，为保证TCP可靠连接，每一个发送的数据段都要加上序列号。建立连接时，两端都会随机生成一个初始序列号。而确认号而是和序列号配合使用的，应答某次请求时，则返回一个确认号，它的值等于对方请求序列号加1 朋友：而6个标志位分别是，URG：这是条紧急信息，ACK:应答消息，PSH:缓冲区尚未填满，RST:重置连接，SYN:建立连接消息标志，FIN：连接关闭通知信息 朋友：窗口大小是接收端用来控制发送端的滑动窗口大小  面试官：那TCP和UDP有什么区别  朋友（松了一口气） 朋友：1）连接方面:TCP面向连接。UDP是无连接的，发送数据之前不需要建立连接 朋友：2）安全方面:TCP提供可靠的服务，保证传送的数据，无差错，不丢失，不重复，且按序到达。UDP则是尽最大努力交付，不保证可靠交付 朋友：3）传输效率：TCP传输效率相对较低，UDP传输效率高  面试官：刚才你说TCP是可靠的连接，它是怎么实现的  朋友：TCP的连接是基于三次握手，而断开则是四次挥手 朋友：为了保障数据不丢失及错误（可靠性），它有报文校验、ACK应答、超时重传(发送方)、失序数据重传（接收方）、丢弃重复数据、流量控制（滑动窗口）和拥塞控制等机制  面试官：具体说一说三次握手和四次挥手机制  朋友（又是常规题，晒晒水啦） 朋友：TCP是可靠的双向通道，所以需要三次握手和四次挥手，我来画个图 三次握手  四次挥手  朋友：提前抢答下，关闭连接时需要四次挥手，比建立时多一次，是因为被动关闭端或许还有数据没被送出去，不能像握手时一样，第二次握手既是发起握手也是响应握手  面试官：如果没有三次握手会有什么问题呢  朋友：如果只有两次握手，client发连接请求后不会再ACK服务端的SYN 朋友：此时若客户端因为自身原因判断建立连接失败，可能会重复建立TCP连接，而服务端却会认为那些被client丢弃的TCP还是有效，会白白浪费资源  面试官：TIME_WAIT和CLOSE_WAIT的区别在哪  朋友：CLOSE_WAIT是被动关闭形成的；当对方close socket而发送FIN报文过来时，回应ACK之后进入CLOSE_WAIT状态。随后检查是否存在未传输数据，如果没有则发起第三次挥手，发送FIN报文给对方，进入LAST_ACK状态并等待对方ACK报文到来 朋友：TIME_WAIT是主动关闭连接方式形成的；处于FIN_WAIT_2状态时，收到对方FIN报文后进入TIME_WAIT状态；之后再等待两个MSL(Maximum Segment Lifetime:报文最大生存时间)  面试官：TIME_WAIT的作用呢，还有为啥状态时间要保持两个MSL  朋友(这问得太深了吧，老哥。还好昨天偷偷补课了) 朋友：1）TIME_WAIT的作用是为了保证最后一次挥手的ACK报文能送达给对方，如果ACK丢失，对方会超时重传FIN，主动关闭端会再次响应ACK过去；如果没有TIME_WAIT状态，直接关闭，对方重传的FIN报文则被响应一个RST报文，此RST会被动关闭端被解析成错误 朋友：2）存在两个连接，第一个连接正常关闭，第二相同的连接紧接着建立；如果第一个连接的迷路报文到来，则会干扰第二连接，等待两个MSL则可以让上次连接的报文数据消逝在网络后  面试官：刚才你还有提到拥塞控制，TCP协议用什么方式去解决拥塞的  朋友：第一方式是慢启动和拥塞避免 朋友：1）慢启动，TCP发送端会维护一个拥塞窗口（congestionwindow）,简称为cwnd。拥塞窗口初始为1个报文段，每经过一次RTT（数据完全发送完到确认的时间），窗口大小翻倍（指数增长，只是前期慢） 朋友：2）拥塞避免，它思路是让拥塞窗口cwnd缓慢增大，发送方的cwnd达到阀值ssthresh(初始值由系统决定的)之后，每经过一个RTT就把拥塞窗口加一，而不是加倍（收到两个或四个确认，都是cwnd+1），cwnd呈线性增加（加法增大） 朋友：（画个图好解析）  朋友：如果遇到网络拥塞，拥塞窗口阀值ssthresh减半，cwnd设置为1，重新进入慢启动阶段  面试官：那拥塞控制还有其他什么方式呢  朋友：快重传和快恢复 朋友：1）快重传是当接收方收到了一个失序的报文，则立马报告给发送方，赶紧重传 朋友：假如接收方M1收到了，M2没有收到，之后的M3、M4、M5又发送了，此时接收方一共连续给发送方反馈了3个M1确认报文。那么快重传规定，发送方只要连续收到3个重复确认，立即重传对方发来的M2（重复确认报文的后一个报文）  朋友：2）快恢复 朋友：当发送方连续收到三个重复确认，ssthresh减半；由于发送方可能认为网络现在没有拥塞，因此与慢启动不同，把cwnd值设置为ssthresh减半之后的值，然后执行拥塞避免算法，cwnd线性增大 朋友：（再来一图）   面试官：知道滑动窗口不，客户端和服务端控制滑动窗口的过程是怎样的  朋友：接收端将自己可以接收的缓冲区大小放入TCP首部中的“窗口大小”字段，通过ACK报文来通知发送端，滑动窗口是接收端用来控制发送端发送数据的大小，从而达到流量控制 朋友：其实发送方的窗口上限，是取值拥塞窗口和滑动窗口两者的最小值  面试官：那你知道滑动窗口和拥塞窗口有什么区别不  朋友：相同点都是控制丢包现象，实现机制都是让发送方发得慢一点 朋友：不同点在于控制的对象不同 朋友：1）流量控制的对象是接收方，怕发送方发的太快，使得接收方来不及处理 朋友：2）拥塞控制的对象是网络，怕发送方发的太快，造成网络拥塞，使得网络来不及处理  面试官：TCP的粘包和拆包问题，你怎么看  朋友：程序需要发送的数据大小和TCP报文段能发送MSS（Maximum Segment Size，最大报文长度）是不一样的 朋友：大于MSS时，而需要把程序数据拆分为多个TCP报文段，称之为拆包；小于时，则会考虑合并多个程序数据为一个TCP报文段，则是粘包；其中MSS = TCP报文段长度-TCP首部长度 朋友：在IP协议层或者链路层、物理层，都存在拆包、粘包现象  面试官：那解决粘包和拆包的方法都有哪些？  朋友：1）在数据尾部增加特殊字符进行分割 朋友：2）将数据定为固定大小 朋友：3）将数据分为两部分，一部分是头部，一部分是内容体；其中头部结构大小固定，且有一个字段声明内容体的大小  面试官：SYN Flood了解吗  朋友：SYN Flood 伪造 SYN 报文向服务器发起连接，服务器在收到报文后用 SYN_ACK 应答，此应答发出去后，不会收到 ACK 报文，造成一个半连接 朋友：若攻击者发送大量这样的报文，会在被攻击主机上出现大量的半连接，耗尽其资源，使正常的用户无法访问，直到半连接超时  面试官：对TCP的掌握挺不错的，下面问下HTTP的知识。你知道一次HTTP请求，程序一般经历了哪几个步骤？  朋友：1）解析域名 -\u0026gt; 2）发起TCP三次握手，建立连接 -\u0026gt; 3）基于TCP发起HTTP请求 -\u0026gt; 4）服务器响应HTTP请求，并返回数据 -\u0026gt; 5）客户端解析返回数据   面试官：HTTP有哪几种响应状态码，列举几个你熟悉的  朋友：大概有以下几种  200：表示成功正常请求 400：语义有误，一般是请求格式不对 401：需求用户验证权限，一般是证书token没通过认证 403：拒绝提供服务 404：资源不存在 500：服务器错误 503：服务器临时维护，过载；可恢复    面试官：不错，再考考你，session和cookie有什么区别  朋友：1）存储位置不同，cookie是保存在客户端的数据；session的数据存放在服务器上 朋友：2）存储容量不同，单个cookie保存的数据小，一个站点最多保存20个Cookie；对于session来说并没有上限 朋友：3）存储方式不同，cookie中只能保管ASCII字符串；session中能够存储任何类型的数据 朋友：4）隐私策略不同，cookie对客户端是可见的；session存储在服务器上，对客户端是透明对 朋友：5）有效期上不同，cookie可以长期有效存在；session依赖于名为JSESSIONID的cookie，过期时间默认为-1，只需关闭窗口该session就会失效 朋友：6）跨域支持上不同，cookie支持跨域名访问；session不支持跨域名访问  面试官：不错，那你了解什么是HTTP分块传送吗  朋友：分块传送是HTTP的一种传输机制，允许服务端发送给客户端的数据分成多个部分，该协议在HTTP/1.1提供  面试官：HTTP分块传送有什么好处  朋友：HTTP分块传输编码允许服务器为动态生成的内容维持HTTP持久连接 朋友：分块传输编码允许服务器在最后发送消息头字段。对于那些头字段值在内容被生成之前无法知道的情形非常重要，例如消息的内容要使用散列进行签名 朋友：HTTP服务器有时使用压缩 （gzip或deflate）以缩短传输花费的时间。分块传输编码可以用来分隔压缩对象的多个部分。在这种情况下，块不是分别压缩的，而是整个负载进行压缩。分块编码有利于一边进行压缩一边发送数据   面试官：HTTP的长连接你怎么理解  朋友：长连接是指客户端和服务建立TCP连接后，它们之间的连接会持续存在，不会因为一次HTTP请求后关闭，后续的请求也是用这个连接 朋友：长连接可以省去TCP的建立和关闭操作，对于频繁请求的客户端适合使用长连接，但是注意恶意的长连接导致服务受损（建议内部服务之间使用）  面试官：HTTP是安全的吗？怎么做到安全的HTTP协议传输  朋友：并非安全，HTTP传输的数据都是明文的，容易被第三方截取；要做安全传输数据，可以使用HTTP的升级版HTTPS协议  面试官：HTTPS和HTTP的区别，你是怎么理解的  朋友：1）http协议的连接是无状态的，明文传输 朋友：2）HTTPS则是由SSL/TLS＋HTTP协议构建的有加密传输、身份认证的网络协议  面试官：SSL/TLS是什么，HTTPS的安全性是怎样实现的？  朋友：SSL(Secure Socket Layer 安全套接层)是基于HTTPS下的一个协议加密层，保障数据私密性。TLS(Transport Layer Security)则是升级版的SSL 朋友：https在http基础加了一层安全认证及加密层TLS或者SSL，它首先会通过安全层进行ca证书认证，正确获取服务端的公钥 朋友：接着客户端会通过公钥和服务端确认一种加密算法，后面的数据则可以使用该加密算法对数据进行加密  面试官：你能详细说下TLS/SSL的认证过程不\u0026hellip;（此时面试官放在桌面的手机震动了起来，他下意识看了看手机，停顿下) 朋友面试暂时告一段落（下回继续）\n参考  腾讯面试HTTP与TCP/IP20连问，你能答出多少 什么是TCP/IP协议？ 太厉害了，终于有人能把TCP/IP协议讲的明明白白了！ TCP的滑动窗口与拥塞窗口  ","id":25,"section":"posts","summary":"前言 最近和一同学聊天，他想换工作，然后去面了一家大厂。当时，他在简历上写着精通TCP/IP，本着对TCP协议稍有了解，面试官也不会深问的想法","tags":["云原生"],"title":"与网络面试官开撕的那几个问题","uri":"https://www.devopsman.cn/1/01/01/","year":"0001"},{"content":"云原生键值数据库TIKV毕业了 云原生键值数据库项目现在在全球拥有近1,000个生产用户\n旧金山，加利福尼亚- 2020年9月2日Cloud Native Computing Foundation®（CNCF®）宣布Tikv是第12个毕业项目，为云原生构建可持续的生态系统。从孵化的阶段过渡到毕业阶段，TiKV已经被越来越多的人采用，它是一个开放的治理过程，成熟的功能以及对社区，可持续性和包容性的坚定承诺。\nTiKV是在Rust中构建的开源分布式事务键值数据库。它提供带有ACID保证的事务键值API。该项目为需要数据持久性，水平可伸缩性，分布式事务，高可用性和强一致性的应用程序提供了统一的分布式存储层，使其成为下一代云原生基础架构的理想数据库。\n\u0026ldquo;TiKV是我们第一个基于Rust的项目之一，并且确实是一个灵活且可扩展的云原生键值存储\u0026rdquo;。云原生计算基金会CTO / COO Chris Aniszczyk说。“自从项目加入CNCF以来，我们对项目的发展以及建立全球开源社区的愿望印象深刻。\n自2018年8月加入CNCF以来，TiKV在生产中的采用率翻了一番，达到了跨多个行业的约1000家公司，核心存储库的参与者已从78人增加到226人，增加了三倍。维护者团队目前由7名成员组成，并且健康分布代表企业包括PingCAP，Zhihu，JD Cloud和Yidian Zixun。\nTiKV的维护者，PingCAP的首席工程师Siddon Tang说：\u0026ldquo;我很高兴看到TiKV从婴儿期成长为CNCF毕业\u0026rdquo;。自2018年加入CNCF以来，该项目和社区已在多个层次上走向成熟，并不断做出贡献，在全球拥有近1000名采用者。\nTiKV从头开始设计为云原生，并且可以很好地集成到现有的CNCF生态系统中。该项目使用Prometheus进行指标报告，并使用gRPC进行通信，并补充了现有的CNCF项目，例如Vitess，etcd，gRPC和Prometheus。它也可以由operator部署在Kubernetes之上，以简化安装，升级和维护。在TiKV之上构建了多个存储系统，包括Prometheus-metrics-in-TiKV项目TiPrometheus。\nTiKV和PingCAP的首席技术官联合创始人Ed Huang说：“五年前我们创建TiKV时，应该不仅仅是TiDB的补充存储而已“。我们将其置于CNCF的管理之下，通过提供可靠，高质量，实用的存储基础来启用和授权下一代数据库。CNCF的毕业证明了我们的愿景和努力。我们期待与TiKV项目进行更多的创新和合作。\n由CNCF资助并由Cure53执行的第三方安全审核已于2020年2月和3月完成，研究小组得出结论。“ TiKV应该被视为已经适当成熟并兑现了其安全承诺。该结论主要源于上述肯定的说明以及总体良好的代码质量和文档。根据2020年2月的评估结果，Cure53可以推荐TiKV进行公有云部署，特别是通过Kubernetes和Prometheus集成到集装箱解决方案中进行监控\nTiKV可以通过TiDB Operator轻松部署在Kubernetes上，并由Prometheus进行监控。该社区还刚刚提供了TiKV-Operator一个Kubernetes Operator的设计和初始实施，该公司旨在自动化TiKV集群的部署，管理和操作任务。\nDailymotion的DevOps团队负责人SmaïneKahlouch说：我们对围绕这个解决方案的生态系统的成熟度感到惊讶。Kubernetes Operator非常容易上手，并可以帮助完成常见的操作任务。具有丰富的可观察性工具。这种级别的弹性可以通过坚如磐石的TiKV实现实现，所以说它的毕业是当之无愧的\n\u0026ldquo;TiKV是一个非常稳定和高性能的分布式键值数据库。它的智能，高效的集群管理功能为我们的在线推荐服务提供了强大的支持\u0026rdquo;。 Yikian Zixun的TiKV维护者和分布式存储工程师Fu Chen说。“我们在2018年采用了TiKV，它已成为我们存储系统的重要组成部分。”\nZhiK.com的TiKV维护者和基础架构主管Sun Xiaoguang表示：“ TiKV为构建通用的云原生状态服务提供了如此出色的基础。TiDB和Zetta Table Store都是基于TiKV构建的，它们共同支持Zhihu的所有大表的应用方案，并解决了由于MySQL的可伸缩性有限而引起的许多问题。我们为成为社区的一员而感到自豪，我们相信TiKV将成为CNCF的一个毕业项目，从而使更多的用户受益。”\nU-Next的高级工程师Birong Huang说：“自2019年12月以来，我们一直在Tier的ARM平台上使用TiKV“。TiKV的可扩展性和高性能使我们能够克服COVID-19带来的不可预测的流量增长。TiKV社区的活跃程度给我们留下了深刻的印象，并愿意为之贡献！祝贺毕业！”\n为了从孵化状态中正式毕业，该项目定义了自己的治理方式，达到了CII最佳实践的合格标准，并采用了CNCF行为准则。\nTiKV背景 TiKV由PingCAP创建，作为TiDB（由同一公司开发的分布式HTAP数据库）的存储后端。TiKV于2018年8月被接受为Sandbox级别的CNCF成员项目，并于2019年4月被接受为孵化项目。\n 2020年5月：最新版本4.0.0 GA 2019年7月1日：发布TiKV 3.0 2019年5月21日，TiKV投票成为CNCF孵化项目 2018年8月28日，TiKV进入CNCF Sandbox 2018年4月27日：发布TiKV 2.0 2017年10月16日：发布TiKV 1.0 2016年10月：TiKV的Beta版已发布并投入生产 2016年4月1日：TiKV开源  ","id":26,"section":"posts","summary":"云原生键值数据库TIKV毕业了 云原生键值数据库项目现在在全球拥有近1,000个生产用户 旧金山，加利福尼亚- 2020年9月2日Cloud Native Computing F","tags":["云原生"],"title":"云原生键值数据库TIKV毕业了","uri":"https://www.devopsman.cn/1/01/01/","year":"0001"}],"tags":[{"title":"devops","uri":"https://www.devopsman.cn/tags/devops/"},{"title":"docker","uri":"https://www.devopsman.cn/tags/docker/"},{"title":"index","uri":"https://www.devopsman.cn/tags/index/"},{"title":"Kubernetes","uri":"https://www.devopsman.cn/tags/kubernetes/"},{"title":"Python","uri":"https://www.devopsman.cn/tags/python/"},{"title":"Redis","uri":"https://www.devopsman.cn/tags/redis/"},{"title":"SQL","uri":"https://www.devopsman.cn/tags/sql/"},{"title":"YAML","uri":"https://www.devopsman.cn/tags/yaml/"},{"title":"云原生","uri":"https://www.devopsman.cn/tags/%E4%BA%91%E5%8E%9F%E7%94%9F/"},{"title":"内核调优","uri":"https://www.devopsman.cn/tags/%E5%86%85%E6%A0%B8%E8%B0%83%E4%BC%98/"},{"title":"博客","uri":"https://www.devopsman.cn/tags/%E5%8D%9A%E5%AE%A2/"},{"title":"安全","uri":"https://www.devopsman.cn/tags/%E5%AE%89%E5%85%A8/"},{"title":"容器","uri":"https://www.devopsman.cn/tags/%E5%AE%B9%E5%99%A8/"},{"title":"工具类","uri":"https://www.devopsman.cn/tags/%E5%B7%A5%E5%85%B7%E7%B1%BB/"},{"title":"年度文章","uri":"https://www.devopsman.cn/tags/%E5%B9%B4%E5%BA%A6%E6%96%87%E7%AB%A0/"},{"title":"消息队列","uri":"https://www.devopsman.cn/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"title":"监控","uri":"https://www.devopsman.cn/tags/%E7%9B%91%E6%8E%A7/"},{"title":"编程","uri":"https://www.devopsman.cn/tags/%E7%BC%96%E7%A8%8B/"},{"title":"网络","uri":"https://www.devopsman.cn/tags/%E7%BD%91%E7%BB%9C/"},{"title":"运维","uri":"https://www.devopsman.cn/tags/%E8%BF%90%E7%BB%B4/"},{"title":"运维技术","uri":"https://www.devopsman.cn/tags/%E8%BF%90%E7%BB%B4%E6%8A%80%E6%9C%AF/"},{"title":"镜像优化","uri":"https://www.devopsman.cn/tags/%E9%95%9C%E5%83%8F%E4%BC%98%E5%8C%96/"}]}